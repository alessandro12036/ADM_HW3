{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adm_hm3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vpHmGKKVr3y"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05q7ZZuI-GDE"
      },
      "source": [
        "def take_n_urls(n):\n",
        "\n",
        "  main_url = \"https://myanimelist.net/topanime.php\"\n",
        "\n",
        "  # this list will contain all the urls we'll retrieve\n",
        "  urls = [] \n",
        "\n",
        "  # each page shows 50 elements and we can retrieve each page by manipulating the \"limit\" query\n",
        "  for limit in range(0, n, 50): \n",
        "    content = requests.get(main_url,\n",
        "                           params={\"limit\": limit})\n",
        "    if content.status_code == 404:\n",
        "      print(f\"Page with limit {limit} was not found. Interrumpting and returning pages found\")\n",
        "      break\n",
        "    soup = bs(content.content, \"html.parser\")\n",
        "\n",
        "    # from the content of each page we retrieve the portions that contain the urls\n",
        "    results = soup.find_all(\"a\", \n",
        "                            class_= \"hoverinfo_trigger fl-l ml12 mr8\")\n",
        "\n",
        "    # finally, we take the string containing each url by taking the attribute href,\n",
        "    # and we append them in the urls list\n",
        "    for result in results:\n",
        "      url = result[\"href\"]\n",
        "      urls.append(url)\n",
        "\n",
        "  return urls"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdJbPl0CWPPq",
        "outputId": "b98b3450-05af-4830-d7dc-e08e66c56092"
      },
      "source": [
        "urls = take_n_urls(20000)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page with limit 19150 was not found. Interrumpting and returning pages found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIe7cbwc_zOt",
        "outputId": "268a24bd-5649-4cce-ff7d-5e9cdf87d09a"
      },
      "source": [
        "# we end up with 19130 urls. \n",
        "# I added a check that tells us when we have exceeded the length of the ranking list and returns what has been found\n",
        "# up until that moment (so to avoid losing any more time with get requests that point to nothing)\n",
        "# I know in the assignment they said 20000 but I'm fairly sure that's all the entries. \n",
        "# This is easy to see if we manually set the limit in the url and check the results. \n",
        "# For example: https://myanimelist.net/topanime.php?limit=15000 contains rankings 15001-15050. The first entry is\n",
        "# Big X Episode 0. If we check our list with urls[15000] (remember that our list is 0-indexed) we obtain the same result.\n",
        "# This to me seems to point to a correct behavior from the function, but let me know what you think.\n",
        "\n",
        "len(urls) \n",
        "print(urls[15000])\n",
        "urls_str = list(map(str, urls))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://myanimelist.net/anime/30839/Big_X_Episode_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwJLUQ69_2Zz"
      },
      "source": [
        "# Since the output of this step has to be a txt file, here we write one with each\n",
        "# url separated by a newline\n",
        "with open(\"urls.txt\", \"w\") as file:\n",
        "  file.write(\"\\n\".join(urls_str))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-HVU_RiAfYf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
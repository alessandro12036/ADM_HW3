{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1vpHmGKKVr3y"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import datetime as dt \n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "05q7ZZuI-GDE"
   },
   "outputs": [],
   "source": [
    "def take_n_urls(n):\n",
    "\n",
    "    main_url = \"https://myanimelist.net/topanime.php\"\n",
    "\n",
    "    # this list will contain all the urls we'll retrieve\n",
    "    urls = [] \n",
    "\n",
    "    # each page shows 50 elements and we can retrieve each page by manipulating the \"limit\" query\n",
    "    for limit in range(0, n, 50): \n",
    "        content = requests.get(main_url,\n",
    "                               params={\"limit\": limit})\n",
    "        if content.status_code == 404:\n",
    "            print(f\"Page with limit {limit} was not found. Interrumpting and returning pages found\")\n",
    "            break\n",
    "        soup = bs(content.content, \"html.parser\")\n",
    "\n",
    "        # from the content of each page we retrieve the portions that contain the urls\n",
    "        results = soup.find_all(\"a\", \n",
    "                                class_= \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "\n",
    "        # finally, we take the string containing each url by taking the attribute href,\n",
    "        # and we append them in the urls list\n",
    "        for result in results:\n",
    "            url = result[\"href\"]\n",
    "            if url not in urls:  # check for duplicates\n",
    "                urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdJbPl0CWPPq",
    "outputId": "b98b3450-05af-4830-d7dc-e08e66c56092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urls...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if \"urls.txt\" not in os.listdir():\n",
    "    urls = take_n_urls(20000)\n",
    "    # Since the output of this step has to be a txt file, here we write one with each\n",
    "    # url separated by a newline\n",
    "    with open(\"urls.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(urls_str))\n",
    "else:\n",
    "    with open(\"urls.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        print(\"Loading urls...\")\n",
    "        urls = file.read().split(\"\\n\")\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIe7cbwc_zOt",
    "outputId": "268a24bd-5649-4cce-ff7d-5e9cdf87d09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19130\n",
      "https://myanimelist.net/anime/30839/Big_X_Episode_0\n"
     ]
    }
   ],
   "source": [
    "# we end up with 19131 urls. \n",
    "# I added a check that tells us when we have exceeded the length of the ranking list and returns what has been found\n",
    "# up until that moment (so to avoid losing any more time with get requests that point to nothing)\n",
    "# I know in the assignment they said 20000 but I'm fairly sure that's all the entries. \n",
    "# This is easy to see if we manually set the limit in the url and check the results. \n",
    "# For example: https://myanimelist.net/topanime.php?limit=15000 contains rankings 15001-15050. The first entry is\n",
    "# Big X Episode 0. If we check our list with urls[15000] (remember that our list is 0-indexed) we obtain the same result.\n",
    "# This to me seems to point to a correct behavior from the function, but let me know what you think.\n",
    "\n",
    "print(len(urls)) \n",
    "print(urls[15000])\n",
    "urls_str = list(map(str, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: the index of article start with 0 and not 1 so all ranks are shifted by 1 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the html pages will be stored\n",
    "if \"html_pages\" not in os.listdir():\n",
    "    os.mkdir(\"html_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t-HVU_RiAfYf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_html_pages(urls):\n",
    "    if \"counter_pages\" not in os.listdir():\n",
    "        start = 0\n",
    "    else:\n",
    "        with open(\"counter_pages\", \"rb\") as counter_file:\n",
    "            start = pickle.load(counter_file) + 1\n",
    "\n",
    "    print(f\"Starting from anime #{start}\")\n",
    "    n = len(urls)\n",
    "    for i in range(start, n):\n",
    "        ranking_page = str(int(np.floor(i/50)))\n",
    "        if i % 50 == 0 or f\"ranking_page_{ranking_page}\" not in os.listdir(\"./html_pages\"):\n",
    "            os.mkdir(f\"html_pages/ranking_page_{ranking_page}\")\n",
    "        html_page = requests.get(urls[i])\n",
    "        sleep_timer = 60\n",
    "        while html_page.status_code != 200: # if the status_code is not 200, we've exceeded the number of requests and have to wait\n",
    "            print(f\"Exceeded number of requests while retrieving page #{i}.\\nWaiting {sleep_timer} seconds\")\n",
    "            html_page.close()\n",
    "            time.sleep(sleep_timer)\n",
    "            html_page = requests.get(urls[i])\n",
    "            sleep_timer += 10\n",
    "        with open (f\"html_pages/ranking_page_{ranking_page}/article_{i}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_page.text)\n",
    "        with open (\"counter_pages\", \"wb\") as counter_file:\n",
    "            pickle.dump(i, counter_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from anime #19130\n"
     ]
    }
   ],
   "source": [
    "save_html_pages(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "1. Anime Name (to save as `animeTitle`): String\n",
    "2. Anime Type (to save as `animeType`): String\n",
    "3. Number of episode (to save as `animeNumEpisode`): Integer\n",
    "4. Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "5. Number of members (to save as `animeNumMembers`): Integer\n",
    "6. Score (to save as `animeScore`): Float\n",
    "7. Users (to save as `animeUsers`): Integer\n",
    "8. Rank (to save as `animeRank`): Integer\n",
    "9. Popularity (to save as `animePopularity`): Integer\n",
    "10. Synopsis (to save as `animeDescription`): String\n",
    "11. Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "12. Characters (to save as `animeCharacters`): List of strings.\n",
    "13. Voices (to save as `animeVoices`): List of strings\n",
    "14. Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ex_aurelie ='C:/Users/aurel/OneDrive/Bureau/IMT/3ème année IMT/0_Cours Sapienza/ADM/Homework/Homework 3'\n",
    "path_ex_alessandro = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the tsv files will be stored\n",
    "if \"tsv_files\" not in os.listdir():\n",
    "    os.mkdir(\"tsv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_info(num_article, folder='tsv_files'):\n",
    "    ranking_page = str(int(np.floor(num_article/50)))\n",
    "    article=f'{path_ex_aurelie}/html_pages/ranking_page_{ranking_page}/article_{num_article}.html'\n",
    "    with open(article, \"r\", encoding=\"utf-8\") as file:\n",
    "        art= bs(file.read(), 'html.parser')\n",
    "    \n",
    "    #animeTitle\n",
    "    animeTitle = art.find('h1', {'class':\"title-name h1_bold_none\"}).string\n",
    "    #print('animeTitle :',animeTitle)\n",
    "    \n",
    "    \n",
    "    #animeType\n",
    "    animeType = art.find('span', {'class':\"information type\"}).string\n",
    "    #print('animeType :',animeType)\n",
    "    \n",
    "    \n",
    "    #animeNumEpisode and Dates (there is not specific name for those two info)\n",
    "    #list lines with tag <div class=\"spaceit_pad\">\n",
    "    lines = art.find_all('div', {'class':\"spaceit_pad\"})\n",
    "    for line in lines :\n",
    "        #for each div tag there is one span, so here we look for the span tag with 'Episodes:' and 'Aired'\n",
    "        sp= line.find('span', {'class':\"dark_text\"})\n",
    "        # to avoid error if there is no span\n",
    "        if sp is not None :\n",
    "            #for span 'Episodes' (and the div tag which corresponds)\n",
    "            if sp.string == 'Episodes:' :\n",
    "                #extract the content of the right div tag and take the third line which correspond to the number of episodes\n",
    "                if line.contents[2] != '\\n  Unknown\\n  ' :\n",
    "                    animeNumEpisode = int(line.contents[2])\n",
    "                    #animeNumEpisode = int(re.findall(r'-?\\d+\\.?\\d*', str(line))[0])           #if we want to use regex  \n",
    "                else :\n",
    "                    animeNumEpisode = ''\n",
    "            #for span 'Aired' (and the div tag which corresponds)\n",
    "            if sp.string == 'Aired:' :\n",
    "                str_dates = line.contents[2].split('\\n  ')[1]\n",
    "                if str_dates == 'Not available':\n",
    "                    releaseDate = ''\n",
    "                    endDate = ''\n",
    "                else :\n",
    "                    #if \"Status: Finished Airing\" (there is a endDate)\n",
    "                    if ('to' in str_dates) and ('?' not in str_dates):\n",
    "                        #extract the content of the right div tag and take the third line which correspond to the dates (fix the issue of '\\n')\n",
    "                        str_releaseDate, str_endDate = str_dates.split(' to ')\n",
    "\n",
    "                        #choose the right datetime format of str_releaseDate \n",
    "                        if len(str_releaseDate.split(' '))==3:\n",
    "                            date_format_releaseDate = \"%b %d, %Y\"\n",
    "                        elif len(str_releaseDate.split(' '))==2:\n",
    "                            date_format_releaseDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_releaseDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        releaseDate = dt.datetime.strptime(str_releaseDate, date_format_releaseDate)\n",
    "\n",
    "                        #choose the right datetime format of str_endDate \n",
    "                        if len(str_endDate.split(' '))==3:\n",
    "                            date_format_endDate = \"%b %d, %Y\"\n",
    "                        elif len(str_endDate.split(' '))==2:\n",
    "                            date_format_endDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_endDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        endDate = dt.datetime.strptime(str_endDate, date_format_endDate)\n",
    "\n",
    "                    else :\n",
    "                        str_releaseDate = str_dates.split(' to ')[0]\n",
    "                        #choose the right datetime format of str_releaseDate \n",
    "                        if len(str_releaseDate.split(' '))==3:\n",
    "                            date_format_releaseDate = \"%b %d, %Y\"\n",
    "                        elif len(str_releaseDate.split(' '))==2:\n",
    "                            date_format_releaseDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_releaseDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        releaseDate = dt.datetime.strptime(str_releaseDate, date_format_releaseDate)\n",
    "\n",
    "                        endDate=''\n",
    "    #print('animeNumEpisode :',animeNumEpisode)\n",
    "    #print('releaseDate :',releaseDate)\n",
    "    #print('endDate :',endDate)\n",
    "    \n",
    "    \n",
    "    #animeNumMembers\n",
    "    animeNumMembers = int(art.find('span', {'class':\"numbers members\"}).contents[1].string.replace(',',''))\n",
    "    #print('animeNumMembers :',animeNumMembers)\n",
    "    \n",
    "    \n",
    "    #animeScore\n",
    "    score = art.find('div', {'class':\"score-label\"}).string\n",
    "    if score == 'N/A':\n",
    "        animeScore = ''\n",
    "    else :\n",
    "        animeScore = float(score)\n",
    "    #print('animeScore :',animeScore)\n",
    "    \n",
    "    \n",
    "    #animeUsers\n",
    "    if art.find('span', {'itemprop':\"ratingCount\"}) is not None :\n",
    "        animeUsers = int(art.find('span', {'itemprop':\"ratingCount\"}).string)\n",
    "    else :\n",
    "        animeUsers = ''\n",
    "    #print('animeUsers :',animeUsers)\n",
    "    \n",
    "    \n",
    "    #animeRank\n",
    "    if art.find('span', {'class':\"numbers ranked\"}).contents[1].string != 'N/A':\n",
    "        animeRank = int(art.find('span', {'class':\"numbers ranked\"}).contents[1].string.replace('#',''))\n",
    "    else :\n",
    "        animeRank =''\n",
    "    #print('animeRank :',animeRank)\n",
    "    \n",
    "    \n",
    "    #animePopularity\n",
    "    animePopularity = int(art.find('span', {'class':\"numbers popularity\"}).contents[1].string.replace('#',''))\n",
    "    #print('animePopularity :',animePopularity)\n",
    "    \n",
    "    \n",
    "    #animeDescription\n",
    "    desc = art.find('p', {'itemprop':\"description\"}).contents\n",
    "    animeDescription=''\n",
    "    #remove <br/> Tag and '\\n'\n",
    "    for ele in desc :\n",
    "        #delete tags with regex \n",
    "        ele = re.sub(re.compile('<.*?>'),'', str(ele))\n",
    "        animeDescription += ele\n",
    "        animeDescription = animeDescription.replace('\\n','')\n",
    "    #print('animeDescription :',animeDescription.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "    #animeRelated\n",
    "    animeRelated = []\n",
    "    #store the table which contain related animes\n",
    "    table = art.find('table', {'class':\"anime_detail_related_anime\"})\n",
    "    if table is not None :\n",
    "        #store all links/anime related with 'a' Tag\n",
    "        links = table.find_all('a')\n",
    "        for link in links :\n",
    "            # check if there is a hyperlink and add it in the list if yes \n",
    "            if (link.get('href') is not None) and (link.string is not None):\n",
    "                animeRelated += [link.string]\n",
    "        animeRelated=list(set(animeRelated))\n",
    "    else :\n",
    "        animeRelated=''\n",
    "    #print('animeRelated :',animeRelated)\n",
    "\n",
    "    \n",
    "    #animeCharacters\n",
    "    animeCharacters = art.find_all('h3', {'class':\"h3_characters_voice_actors\"})\n",
    "    animeCharacters = [char.string for char in animeCharacters]\n",
    "    #print('animeCharacters :',animeCharacters)\n",
    "    \n",
    "    \n",
    "    #animeVoices\n",
    "    td_Voices = art.find_all('td', {'class':\"va-t ar pl4 pr4\"})\n",
    "    animeVoices = [voice.find('a').string for voice in td_Voices]\n",
    "    #print('animeVoices :',animeVoices)\n",
    "    \n",
    "    \n",
    "    #animeStaff\n",
    "    #if there is a staff, the div which correspond to the table Staff is the second one (there are div with {'class':\"detail-characters-list clearfix\"})\n",
    "    if len(art.find_all('div', {'class':\"detail-characters-list clearfix\"}))>1 :\n",
    "        div_staff = art.find_all('div', {'class':\"detail-characters-list clearfix\"})[1] \n",
    "        td_staff = div_staff.find_all('td', {'class':\"borderClass\"})\n",
    "        animeStaff=[]\n",
    "        for td in td_staff :\n",
    "            if td.get('width') == None:\n",
    "                animeStaff.append([td.find('a').string,td.find('small').string])\n",
    "    #if there is not staff\n",
    "    else :\n",
    "        animeStaff = ''\n",
    "    #print('animeStaff :',animeStaff)\n",
    "    \n",
    "    #create a .tsv file with attributes\n",
    "    with open(f'{folder}/anime_{num_article}', 'wt', encoding=\"utf8\") as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, \\\n",
    "                            animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \\\n",
    "                            animeCharacters, animeVoices, animeStaff])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(17346, len(urls)):\n",
    "    collect_info(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Pre-processing\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aurel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stemming\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "\n",
    "# For identifying the stop words\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    try:\n",
    "        tokenized = nltk.word_tokenize(text)\n",
    "        stemmed = [porter.stem(word) for word in tokenized if ((word.lower() not in eng_stopwords) and (word not in string.punctuation))]\n",
    "    except TypeError as e:\n",
    "        print(text)\n",
    "        raise TypeError\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# To sort the files correctly\n",
    "def alphanumeric_sort(key):\n",
    "    num = int(re.search(\"([0-9]+)\", key).group(0))\n",
    "    return num\n",
    "\n",
    "def merge_tsvs(path, colnames):\n",
    "    files = sorted(os.listdir(path), key=alphanumeric_sort)\n",
    "    df = pd.read_csv(path+files[0],\n",
    "                     names=colnames,\n",
    "                     sep=\"\\t\", engine='python')\n",
    "    for file_name in files[1:]:\n",
    "        df2 = pd.read_csv(path+file_name,\n",
    "                          names=colnames,\n",
    "                          sep=\"\\t\", engine='python')\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the implemented sorting algorithm\n",
    "#print(sorted(os.listdir(\"./tsv_files/\"), key=alphanumeric_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./tsv_files/\"\n",
    "colnames = [\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\", \"animeNumMembers\",\n",
    "            \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\", \"animeDescription\", \"animeRelated\",\n",
    "            \"animeCharacters\", \"animeVoices\", \"animeStaff\"]\n",
    "df = merge_tsvs(path, colnames)\n",
    "\n",
    "# Save our df in csv format\n",
    "df.to_csv(\"./html_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of our dataframe with an extra column containing the preprocessed synopsis\n",
    "df_new = df.assign(tokenized_desc=df[\"animeDescription\"].apply(lambda x: process_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19130 entries, 0 to 19129\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   animeTitle        19130 non-null  object \n",
      " 1   animeType         19130 non-null  object \n",
      " 2   animeNumEpisode   18620 non-null  float64\n",
      " 3   releaseDate       18760 non-null  object \n",
      " 4   endDate           8451 non-null   object \n",
      " 5   animeNumMembers   19130 non-null  int64  \n",
      " 6   animeScore        13436 non-null  float64\n",
      " 7   animeUsers        13436 non-null  float64\n",
      " 8   animeRank         17307 non-null  float64\n",
      " 9   animePopularity   19130 non-null  int64  \n",
      " 10  animeDescription  19130 non-null  object \n",
      " 11  animeRelated      12706 non-null  object \n",
      " 12  animeCharacters   19130 non-null  object \n",
      " 13  animeVoices       19130 non-null  object \n",
      " 14  animeStaff        10247 non-null  object \n",
      " 15  tokenized_desc    19130 non-null  object \n",
      "dtypes: float64(4), int64(2), object(10)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df_with_tokens.p\", \"wb\") as file:\n",
    "    pickle.dump(df_new, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "#### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First i chose to create the vocabulary file as a DataFrame store as a .csv file \n",
    "def create_vocabulary(corpus, name_voc_file = \"vocabulary.pkl\"):\n",
    "    voc = set()\n",
    "    i=0\n",
    "    for doc in corpus :\n",
    "        #print(i)\n",
    "        #i+=1\n",
    "        voc = voc.union(set(doc))\n",
    "\n",
    "    dict_voc = dict(zip(sorted(voc),range(len(voc))))\n",
    "    with open(name_voc_file, \"wb\") as file:\n",
    "        pickle.dump(dict_voc, file)\n",
    "    return dict_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(corpus,voc, name_inv_ind_file = \"inverted_index.pkl\"):\n",
    "    #create a inverted_index \"empty\", i.e. only with term_id of vocabulary\n",
    "    inverted_index = dict()\n",
    "    for term, term_id in voc.items() :\n",
    "        inverted_index[term_id]=set()\n",
    "        #inverted_index['term_id_'+str(term_id)]=set()\n",
    "\n",
    "    for doc, num_doc in zip(corpus,range(len(corpus))) :\n",
    "        #print(num_doc)\n",
    "        words_checked = []\n",
    "        for word in doc :\n",
    "            if word not in words_checked: # to avoid looking for the same word more than once in the same doc\n",
    "                words_checked.append(word)\n",
    "                term_id = voc[word]\n",
    "                inverted_index[term_id]=inverted_index[term_id].union(set([num_doc]))\n",
    "                #inverted_index['term_id_'+str(term_id)]=inverted_index['term_id_'+str(term_id)].union(set(['document_'+str(num_doc)]))\n",
    "    \n",
    "    for term_id, docs in inverted_index.items() :\n",
    "        inverted_index[term_id]=sorted(list(inverted_index[term_id]))\n",
    "    \n",
    "    #save the inverted_index as a .pkl file\n",
    "    with open(name_inv_ind_file, \"wb\") as file:\n",
    "        pickle.dump(inverted_index,file)\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [0, 1], 2: [0], 3: [1], 4: [0]}\n"
     ]
    }
   ],
   "source": [
    "#test with a simple case\n",
    "\n",
    "L=['A','C', 'B', 'A', 'E']\n",
    "corpus_test=[L]+[['B','D']]\n",
    "create_vocabulary(corpus_test)\n",
    "\n",
    "with open(\"vocabulary.pkl\", \"rb\") as file:\n",
    "    voc_test = pickle.load(file)\n",
    "        \n",
    "inverted_index(corpus_test,voc_test)\n",
    "\n",
    "with open(\"inverted_index.pkl\", \"rb\") as file:\n",
    "    inv_ind_test = pickle.load(file)\n",
    "print(inv_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset of synopsis stored in df_with_tokens.p\n",
    "def download_corpus(name_file_corpus = 'df_with_tokens.p'):\n",
    "    with open(name_file_corpus, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "\n",
    "    corpus = list(df['tokenized_desc'])\n",
    "    print(len(corpus))\n",
    "    return corpus\n",
    "\n",
    "#download the voc or create it if it does not already exist\n",
    "def download_voc(corpus, name_voc_file):\n",
    "    if name_voc_file not in os.listdir():\n",
    "        voc = create_vocabulary(corpus, name_voc_file)\n",
    "    else :\n",
    "        with open(name_voc_file, \"rb\") as file:\n",
    "            voc = pickle.load(file)\n",
    "    return voc\n",
    "\n",
    "#download the voc or create it if it does not already exist\n",
    "def download_inverted_index(corpus,voc, name_inv_ind_file):\n",
    "    if name_inv_ind_file not in os.listdir():\n",
    "        inv_ind = inverted_index(corpus,voc, name_inv_ind_file)\n",
    "    else :\n",
    "        with open(name_inv_ind_file, \"rb\") as file:\n",
    "            inv_ind = pickle.load(file)\n",
    "    return inv_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19130\n"
     ]
    }
   ],
   "source": [
    "#define the names of the interesting files \n",
    "name_voc_file = \"vocabulary.pkl\"\n",
    "name_file_corpus = 'df_with_tokens.p'\n",
    "name_inv_ind_file = \"inverted_index.pkl\"\n",
    "\n",
    "#download the corpus and the vocabulary\n",
    "corpus = download_corpus(name_file_corpus)\n",
    "voc = download_voc(corpus, name_voc_file)\n",
    "inv_ind = download_inverted_index(corpus,voc, name_inv_ind_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "```saiyan race```\n",
    "\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `animeTitle`\n",
    "* `animeDescription`\n",
    "* `Url`\n",
    "\n",
    "__Example Output__:\n",
    "\n",
    "| animeTitle | animeDescription | Url |\n",
    "|:-----------------------------:|:-----:|:------------------------------------------------------------:|\n",
    "| Fullmetal Alchemist: Brotherhood | ... | https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood |\n",
    "| Gintama | ... | https://myanimelist.net/anime/28977/Gintama%C2%B0 |\n",
    "| Shingeki no Kyojin Season 3 Part 2 | ... | https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2 |\n",
    "\n",
    "If everything works well in this step, you can go to the next point, and make your Search Engine more complex and better in answering queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(voc, inverted_index):\n",
    "    #ask the query to the user\n",
    "    query = input('What is your query ?').split(' ')\n",
    "    \n",
    "    first_word = query[0]\n",
    "    \n",
    "    if first_word in voc :\n",
    "        first_term_id = voc[first_word]\n",
    "        docs_list = set(inverted_index[first_term_id])\n",
    "        for word in query[1:] :\n",
    "            if word in voc :\n",
    "                term_id = voc[word]\n",
    "                docs = inverted_index[term_id]\n",
    "                docs_list = docs_list.intersection(set(inverted_index[first_term_id]))\n",
    "            else :\n",
    "                print('Nothing correspond to your queries')\n",
    "                return\n",
    "        \n",
    "        #Now we have the doc IDs so we can merge interesting information \n",
    "        html_df = pd.read_csv(path_ex_aurelie+\"/html_df.csv\") #csv which contains tsv line of each document\n",
    "        cols = [\"animeTitle\", \"animeDescription\"]\n",
    "        result = html_df.iloc[sorted(list(docs_list))][cols]\n",
    "        result['Url'] = [urls[i] for i in sorted(list(docs_list))]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    else :\n",
    "        print('Nothing correspond to your queries')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your query ?saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>Dragon Ball Super</td>\n",
       "      <td>Seven years after the events of Dragon Ball Z,...</td>\n",
       "      <td>https://myanimelist.net/anime/30694/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>Dragon Ball Z Movie 14: Kami to Kami</td>\n",
       "      <td>Following the defeat of a great adversary, Gok...</td>\n",
       "      <td>https://myanimelist.net/anime/14837/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...</td>\n",
       "      <td>As Goku investigates the destruction of the So...</td>\n",
       "      <td>https://myanimelist.net/anime/901/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>Dragon Ball: Ossu! Kaettekita Son Gokuu to Nak...</td>\n",
       "      <td>Based on an original concept by the original a...</td>\n",
       "      <td>https://myanimelist.net/anime/5152/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>Dragon Ball Z Movie 10: Kiken na Futari! Super...</td>\n",
       "      <td>After his loss to Goku, Broly crash lands and ...</td>\n",
       "      <td>https://myanimelist.net/anime/903/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>Dragon Ball Z: Summer Vacation Special</td>\n",
       "      <td>One peaceful afternoon, the Son family and fri...</td>\n",
       "      <td>https://myanimelist.net/anime/22695/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>Dragon Ball Z: Zenbu Misemasu Toshi Wasure Dra...</td>\n",
       "      <td>In this film, which is believed to take place ...</td>\n",
       "      <td>https://myanimelist.net/anime/22699/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6560</th>\n",
       "      <td>Galo Sengen</td>\n",
       "      <td>Galo Sengen (\"Galish Man GAL\" in English / GAL...</td>\n",
       "      <td>https://myanimelist.net/anime/20929/Galo_Sengen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>Dragon Ball Z: The Real 4-D at Super Tenkaichi...</td>\n",
       "      <td>Dragon Ball Z: The Real 4-D at Super Tenkaichi...</td>\n",
       "      <td>https://myanimelist.net/anime/42449/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9229</th>\n",
       "      <td>Dragon Ball Z Movie 11: Super Senshi Gekiha!! ...</td>\n",
       "      <td>Jaga Bada, Mr. Satan's old sparring partner, h...</td>\n",
       "      <td>https://myanimelist.net/anime/904/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "364                                       Dragon Ball Z   \n",
       "401                            Dragon Ball Super: Broly   \n",
       "1035                                    Dragon Ball Kai   \n",
       "1467  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1961                                  Dragon Ball Super   \n",
       "2015               Dragon Ball Z Movie 14: Kami to Kami   \n",
       "2296  Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...   \n",
       "4339  Dragon Ball: Ossu! Kaettekita Son Gokuu to Nak...   \n",
       "4673  Dragon Ball Z Movie 10: Kiken na Futari! Super...   \n",
       "5664             Dragon Ball Z: Summer Vacation Special   \n",
       "6092  Dragon Ball Z: Zenbu Misemasu Toshi Wasure Dra...   \n",
       "6560                                        Galo Sengen   \n",
       "8969  Dragon Ball Z: The Real 4-D at Super Tenkaichi...   \n",
       "9229  Dragon Ball Z Movie 11: Super Senshi Gekiha!! ...   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "364   Five years after winning the World Martial Art...   \n",
       "401   Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1035  Five years after the events of Dragon Ball, ma...   \n",
       "1467  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1961  Seven years after the events of Dragon Ball Z,...   \n",
       "2015  Following the defeat of a great adversary, Gok...   \n",
       "2296  As Goku investigates the destruction of the So...   \n",
       "4339  Based on an original concept by the original a...   \n",
       "4673  After his loss to Goku, Broly crash lands and ...   \n",
       "5664  One peaceful afternoon, the Son family and fri...   \n",
       "6092  In this film, which is believed to take place ...   \n",
       "6560  Galo Sengen (\"Galish Man GAL\" in English / GAL...   \n",
       "8969  Dragon Ball Z: The Real 4-D at Super Tenkaichi...   \n",
       "9229  Jaga Bada, Mr. Satan's old sparring partner, h...   \n",
       "\n",
       "                                                    Url  \n",
       "364     https://myanimelist.net/anime/813/Dragon_Ball_Z  \n",
       "401   https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "1035  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "1467  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "1961  https://myanimelist.net/anime/30694/Dragon_Bal...  \n",
       "2015  https://myanimelist.net/anime/14837/Dragon_Bal...  \n",
       "2296  https://myanimelist.net/anime/901/Dragon_Ball_...  \n",
       "4339  https://myanimelist.net/anime/5152/Dragon_Ball...  \n",
       "4673  https://myanimelist.net/anime/903/Dragon_Ball_...  \n",
       "5664  https://myanimelist.net/anime/22695/Dragon_Bal...  \n",
       "6092  https://myanimelist.net/anime/22699/Dragon_Bal...  \n",
       "6560    https://myanimelist.net/anime/20929/Galo_Sengen  \n",
       "8969  https://myanimelist.net/anime/42449/Dragon_Bal...  \n",
       "9229  https://myanimelist.net/anime/904/Dragon_Ball_...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_1(voc, inv_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(word, doc, corpus, idf=None):\n",
    "    tf = doc.count(word) / len(doc)\n",
    "    counter_docs = 0\n",
    "    # if the idf parameter has not been provided, we compute it\n",
    "    if idf == None:\n",
    "        for text in corpus:\n",
    "            if word in text:\n",
    "                counter_docs += 1\n",
    "        idf = np.log(len(corpus) / counter_docs)\n",
    "    tfidf = tf * idf\n",
    "    return idf, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_inverted_index(corpus, voc):\n",
    "    inverted_index_2 = dict()\n",
    "    # first, we initialize each field in the inverted_index\n",
    "    for term_id in voc.values() :\n",
    "        inverted_index_2[term_id]=list()\n",
    "        \n",
    "    idf_calculated = {} # the idfs can be calculated once for each word since idf = np.log(len(corpus) / documents_with_word)\n",
    "    \n",
    "    for doc, num_doc in zip(corpus,range(len(corpus))) :\n",
    "        words_checked = []\n",
    "        for word in doc:\n",
    "            if word not in words_checked: # to avoid looking for the same word more than once in the same doc\n",
    "                term_id = voc[word]\n",
    "                # if this is the first time we encounter this word, we need to obtain the idf and save it for future use\n",
    "                if word not in idf_calculated.keys():\n",
    "                    idf, tfidf = get_tfidf(word, doc, corpus)\n",
    "                    idf_calculated[word] = idf\n",
    "                # otherwise, we provide it to the function directly\n",
    "                else:\n",
    "                    _, tfidf = get_tfidf(word, doc, corpus, idf)\n",
    "                # we add the doc index and the tfidf score to the dictionary\n",
    "                inverted_index_2[term_id].append([num_doc, tfidf])\n",
    "                # we mark this word as \"checked\" for this document\n",
    "                words_checked.append(word)\n",
    "    \n",
    "    # we order the items by tfidf score for that term\n",
    "    for term_id, docs in inverted_index_2.items() :\n",
    "        inverted_index_2[term_id]=sorted(inverted_index_2[term_id], key=lambda x: x[1])\n",
    "    \n",
    "    # finally we save the item in order to avoid having to create the index again\n",
    "    with open (\"inverted_index_2.p\", \"wb\") as file:\n",
    "        pickle.dump(inverted_index_2, file)\n",
    "    return inverted_index_2, idf_calculated # we also return the calculated idfs so to avoid calculating them over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"inverted_index_2.p\" not in os.listdir():\n",
    "    ii_2, idfs = second_inverted_index(corpus, voc)\n",
    "else:\n",
    "    with open (\"inverted_index_2.p\", \"rb\") as file:\n",
    "        ii_2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    num = np.dot(vec1, vec2)\n",
    "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    cos = num / denom\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# implement the case in which the query is not in any document\n",
    "\n",
    "def search_k_matches(query, corpus, voc, ii, idfs, k=10):\n",
    "    df = pd.read_csv(\"./html_df.csv\")\n",
    "    query = process_text(query.lower())\n",
    "    dict_relevant = {}\n",
    "    for word in query:\n",
    "        if word in voc.keys(): # checks if query is in our vocabulary\n",
    "            term_id = voc[word]\n",
    "            for doc_info in ii[term_id]:\n",
    "                if doc_info[0] not in dict_relevant.keys():\n",
    "                    dict_relevant[doc_info[0]] = []\n",
    "                dict_relevant[doc_info[0]].append(doc_info[1])\n",
    "    len_query = len(query)\n",
    "    query_vector = [(query.count(x) / len_query) * idfs[x] for x in query if x in idfs.keys()] # we treat the query as a document\n",
    "    distances = []\n",
    "    for key in dict_relevant.keys():\n",
    "        vector = dict_relevant[key]\n",
    "        if len(vector) == len(query_vector): # this assures the conjuctive (and) property of the search engine\n",
    "            distances.append((-cosine_similarity(query_vector, vector), key)) # negative of cosine_similarity to get max heap\n",
    "    heapq.heapify(distances)\n",
    "    n_matches = len(distances)\n",
    "    final_results = []\n",
    "    for i in range(min(k, n_matches)):\n",
    "        el = heapq.heappop(distances)\n",
    "        final_results.append([el[1], -el[0]]) # make the cosine distance positive again for the output\n",
    "    \n",
    "    #print(final_results)\n",
    "    indices = [x[0] for x in final_results]\n",
    "    distances = [x[1] for x in final_results]\n",
    "    cols = [\"animeTitle\", \"animeDescription\"]\n",
    "    partial_df = df.iloc[indices][cols]\n",
    "    final_df = partial_df.assign(Url=[urls[i] for i in indices],\n",
    "                                 Similarity=distances)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test it against the keywords \"ranma urusei\" which are two characters from two different animes that only meet in a special crossover episode called \"It's a Rumic World\". We can see that the search engine correctly gives this result as the best match, followed by another Ranma 1/2 episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ranma urusei\"\n",
    "output = search_k_matches(query, corpus, voc, ii_2, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.997792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.910875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.997792  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.910875  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Currently not working, cosine similarity doesn't behave well with 0s, will try to think of a solution tomorrow morning***\n",
    "\n",
    "The idea here is to obtain a query with several parameters, like for example year of release, voice actors, genre and so on. This query can be obtained either directly in the format \"query [parameter1=x, parameter2=y...]\" or through a form that assembles the same string so that the inner logic of the algorithm doesn't change either way.<br><br>\n",
    "The function process_query then builds a dictionary for the query by extrapolating the parameters through regular expressions and returns this dictionary to the main function.<br><br>\n",
    "Now, in order to come up with some score for the matches in this parameters, I thought of using the cosine distance again and treat the parameters as additional dimensions. Meaning that if in the previous search engine (the one with the tfidf) we had for each doc a vector of tfidfs, now we have the same thing but also ones or zeros depending if there is a match in the parameters (i.e.: [3.60997786640952, 4.1247875750000995, 1, 0, 1, 1]).\n",
    "Then we compute the cosine distance from the query as before (and we treat the query as its own document again).\n",
    "This way we have a cosine distance that takes into account the additional parameters.<br><br>\n",
    "It's very rough, and it just allows for a binary definition of the parameters (either they match or they don't). I know that for certain parameters we could probably achieve a more refined scoring, but it seemed like a lot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thinking of a query like \"sayian race [year=1999 episodes=22]\". So first main query and then in squared parentheses\n",
    "# the parameters.\n",
    "\n",
    "# processes the query in its single components\n",
    "def process_query(query):\n",
    "    query_dict = dict()\n",
    "    main_query = re.search(\"^(.+)\\[\", query)\n",
    "    anime_voices = re.search(\"voices=\\(([^\\)]+)\\)\", query)\n",
    "    anime_chars = re.search(\"characters=\\(([^\\)]+)\\)\", query)\n",
    "    anime_related = re.search(\"related=\\(([^\\)]+)\\)\", query)\n",
    "    year = re.search(\"year=([0-9]+)[,\\]]\", query)\n",
    "    anime_type = re.search(\"type=([a-zA-Z]+)[,\\]]\", query)\n",
    "    \n",
    "    if year:\n",
    "        query_dict[\"release_year\"] = year.groups()[0]\n",
    "    if anime_type:\n",
    "        query_dict[\"anime_type\"] = anime_type.groups()[0]\n",
    "    # transform these fields in lists before putting them in the dictionary\n",
    "    if anime_voices:\n",
    "        anime_voices = anime_voices.groups()[0]\n",
    "        query_dict[\"anime_voices\"] = [x for x in anime_voices.strip().split(\",\")]\n",
    "    if anime_chars:\n",
    "        anime_chars = anime_chars.groups()[0]\n",
    "        query_dict[\"anime_characters\"] = [x for x in anime_chars.strip().split(\",\")]\n",
    "    if anime_related:\n",
    "        anime_related = anime_related.groups()[0]\n",
    "        query_dict[\"anime_related\"] = [x for x in anime_related.strip().split(\",\")]\n",
    "    \n",
    "    # preprocesses main query before putting it in the dictionary\n",
    "    query_dict[\"main_text_query\"] = process_text(main_query.groups()[0])\n",
    "    #print(query_dict)\n",
    "    \n",
    "    return query_dict\n",
    "    \n",
    "\n",
    "# we need to treat the main query as it was its own document (I found references online on this).\n",
    "# This is the same thing i implemented in 2.2.2, I just put it in a function here.\n",
    "def evaluate_main_query(query, corpus, voc, ii, idfs):\n",
    "    dict_relevant = {}\n",
    "    for word in query:\n",
    "        if word in voc.keys(): # checks if query is in our vocabulary\n",
    "            term_id = voc[word]\n",
    "            for doc_info in ii[term_id]:\n",
    "                if doc_info[0] not in dict_relevant.keys():\n",
    "                    dict_relevant[doc_info[0]] = []\n",
    "                dict_relevant[doc_info[0]].append(doc_info[1])\n",
    "                \n",
    "    return dict_relevant\n",
    "    \n",
    "# Here we append to the vector made of tfidfs the extra values obtained from the parameterized queries\n",
    "def evaluate_parameters(query_d, vector, df, anime_num):\n",
    "    relevant_row = df.iloc[anime_num]\n",
    "    for dict_key in sorted(query_d.keys()):\n",
    "        if dict_key == \"release_year\":\n",
    "            year = datetime.datetime.strptime(relevant_row[\"releaseDate\"], \"%Y-%m-%d %H:%M:%S\").year\n",
    "            vector.append(int(str(year) == query_d[dict_key])) # evaluates the boolean to an integer\n",
    "            \n",
    "        elif dict_key == \"anime_type\":\n",
    "            anime_type = relevant_row[\"animeType\"]\n",
    "            vector.append(int(anime_type.lower() == query_d[dict_key].lower()))\n",
    "            \n",
    "        elif dict_key == \"anime_characters\":\n",
    "            chars = relevant_row[\"animeCharacters\"].lower()\n",
    "            matches = 0\n",
    "            for char_query in query_d[dict_key]:\n",
    "                char_query = char_query.lower()\n",
    "                if char_query in chars:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "            \n",
    "        elif dict_key == \"anime_related\":\n",
    "            related = relevant_row[\"animeRelated\"].lower()\n",
    "            matches = 0\n",
    "            for rel_anime_query in query_d[dict_key]:\n",
    "                rel_anime_query = rel_anime_query.lower()\n",
    "                if rel_anime_query in related:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "            \n",
    "        elif dict_key == \"anime_voices\":\n",
    "            voices = relevant_row[\"animeVoices\"].lower()\n",
    "            matches = 0\n",
    "            for voices_query in query_d[dict_key]:\n",
    "                voices_query = voices_query.lower()\n",
    "                if voices_query in voices:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "        \n",
    "    return vector\n",
    "\n",
    "# Here we obtain the query via form\n",
    "def get_query_with_form():\n",
    "    query_d = dict()\n",
    "    main_query = input(\"Enter your query: \")\n",
    "    year = input(\"Year it was released: \")\n",
    "    anime_type = input(\"Type of anime: \")\n",
    "    voices = input(\"Voice actors: \")\n",
    "    characters = input(\"Characters: \")\n",
    "    related = input(\"Related animes: \")\n",
    "    \n",
    "    query_string = (f\"{main_query} [year={year}, anime_type={anime_type}, related=({related}),\"\n",
    "                    f\"voices=({voices}), characters=({characters})]\")\n",
    "    \n",
    "    #print(query_string)\n",
    "    \n",
    "    return query_string\n",
    "\n",
    "\n",
    "def search_k_matches_2(corpus, voc, ii, idfs, query=None, k=10):\n",
    "    df = pd.read_csv(\"./html_df.csv\")\n",
    "    if not query:\n",
    "        query = get_query_with_form()\n",
    "    query_dict = process_query(query)\n",
    "    # here we process the main query as it was its own document with perfect match parameters.\n",
    "    main_query = query_dict[\"main_text_query\"]\n",
    "    len_query = len(main_query)\n",
    "    query_vector = [(main_query.count(x) / len_query) * idfs[x] for x in main_query if x in idfs.keys()] # we treat the query as a document\n",
    "    query_vector += [1]*(len(query_dict.keys())-1)\n",
    "    #print(query_vector)\n",
    "    \n",
    "    distances = []\n",
    "    dict_relevant = evaluate_main_query(main_query, corpus, voc, ii, idfs)\n",
    "    \n",
    "    for key in dict_relevant.keys():\n",
    "        vector = dict_relevant[key]\n",
    "        vector = evaluate_parameters(query_dict, vector, df, key)\n",
    "        if len(vector) == len(query_vector): # this assures the conjuctive (and) property of the search engine\n",
    "            distances.append((-cosine_similarity(query_vector, vector), key)) # negative of cosine_similarity to get max heap\n",
    "    heapq.heapify(distances)\n",
    "    n_matches = len(distances)\n",
    "    final_results = []\n",
    "    for i in range(min(k, n_matches)):\n",
    "        el = heapq.heappop(distances)\n",
    "        final_results.append([el[1], -el[0]]) # make the cosine distance positive again for the output\n",
    "    \n",
    "    #print(final_results)\n",
    "    indices = [x[0] for x in final_results]\n",
    "    distances = [x[1] for x in final_results]\n",
    "    cols = [\"animeTitle\", \"animeDescription\"]\n",
    "    partial_df = df.iloc[indices][cols]\n",
    "    final_df = partial_df.assign(Url=[urls[i] for i in indices],\n",
    "                                 Similarity=distances)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.536458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.357107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.536458  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.357107  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with query provided\n",
    "query = \"ranma urusei [year=2008, type=special, voices=(gianni), related=(gemelli del destino), characters=(ranma)]\"\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: ranma urusei\n",
      "Year it was released: 2010\n",
      "Type of anime: special\n",
      "Voice actors: gianni\n",
      "Characters: ranma\n",
      "Related animes: \n",
      "[3.60997786640952, 4.1247875750000995, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.539927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.362313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.539927  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.362313  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example qith query via form\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: ranma urusei\n",
      "Year it was released: 2010\n",
      "Type of anime: sdventure\n",
      "Voice actors: gianni\n",
      "Characters: sandro\n",
      "Related animes: \n",
      "[3.60997786640952, 4.1247875750000995, 1, 1, 1, 1]\n",
      "[0.15928001406847617, 0.0, 0, 0.0, 0]\n",
      "[0.16162316495793932, 0.0, 0, 0.0, 0]\n",
      "[0.17991495165333934, 0.0, 0, 0.0, 0]\n",
      "[0.19668099734300482, 0.0, 0, 0.0, 0]\n",
      "[0.2136123731528463, 0.09705382529411999, 0.0, 0, 0.0, 0]\n",
      "[0.23473840624843567, 0.0, 0, 0.0, 0]\n",
      "[0.2499871257575818, 0.0, 0, 0.0, 0]\n",
      "[0.32863376874781, 0.0, 0, 0.0, 0]\n",
      "[0.3521076093726535, 0.3521076093726535, 0.0, 0, 0.0, 0]\n",
      "[0.3683650884091347, 0.0, 0, 0.0, 0]\n",
      "[0.39436052249737197, 0.0, 0, 0.0, 0]\n",
      "[0.4579847946656437, 0.0, 0, 0.0, 0]\n",
      "[0.4579847946656437, 0.0, 0, 0.0, 0]\n",
      "[1.0563228281179604, 0.0, 0, 0.0, 0]\n",
      "[0.16529058063709792, 0.0, 0, 0.0, 0]\n",
      "[0.24647532656085747, 0.0, 0, 0.0, 0]\n",
      "[0.30809415820107183, 0.0, 0, 0.0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.937347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.855695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.937347  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.855695  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_k_matches_2(corpus, voc, ii_2, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                       4286\n",
       "animeTitle          It's a Rumic World: 50th Anniversary Weekly★Sh...\n",
       "animeType                                                     Special\n",
       "animeNumEpisode                                                   1.0\n",
       "releaseDate                                       2008-07-30 00:00:00\n",
       "endDate                                           2008-08-11 00:00:00\n",
       "animeNumMembers                                                  3682\n",
       "animeScore                                                       6.88\n",
       "animeUsers                                                     1914.0\n",
       "animeRank                                                      4286.0\n",
       "animePopularity                                                  7858\n",
       "animeDescription    The characters from Ranma 1/2, Urusei Yatsura,...\n",
       "animeRelated                ['Ranma ½', 'InuYasha', 'Urusei Yatsura']\n",
       "animeCharacters     ['Inuyasha', 'Higurashi, Kagome', 'Saotome, Ra...\n",
       "animeVoices         ['Yamaguchi, Kappei', 'Yukino, Satsuki', 'Haya...\n",
       "animeStaff          [['Yokote, Michiko', 'Script'], ['Takahashi, R...\n",
       "Name: 4286, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4286]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a *back-to-back sequence* of requests for appointments. A sequence of requests is of the form\n",
    "    > 30, 40, 25, 50, 30, 20\n",
    "where each number is the time that the person who makes the appointment wants to spend.\n",
    "You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, `[30, 50, 20]` is an acceptable solution (of duration *100*), but `[30, 40, 50, 20]` is not, because *30* and *40* are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration *110*.\n",
    "1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "\n",
    "##### Here we consider that all values in the instance are unique and len(instance) = n\n",
    "\n",
    "To compute the acceptable solution with the longest possible duration, we have to follow several steps :\n",
    "1. Compute every possible solution : So for that, list all sublists which represent each possible list of appointments.\n",
    "2. For each sublist, tell if it is acceptable or not, so if there are two consecutive appointments or not.\n",
    "3. Compute the total duration of each acceptable solution.\n",
    "4. Finally, return the solution which correspond to the maximum duration.\n",
    "\n",
    "```\n",
    "Input: \n",
    "    instance: list of length n\n",
    "\n",
    "function optimal_solution(instance):\n",
    "    n=len(instance)\n",
    "    for i=0 to n: \n",
    "        sublists = sublists + [all sublists with i elements]\n",
    "    \n",
    "    acceptable_solutions=[all element of sublists which are acceptable]\n",
    "    \n",
    "    durations = [duration of each element of acceptable_solutions]\n",
    "    max_duration = max(durations)\n",
    "    optimal_solutions = [sublists of instance with total duration == max_durations]\n",
    "    \n",
    "    return optimal_solutions, max_duration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n",
    "\n",
    "First of all, we create a function ```is_acceptable(solution, instance)``` that says if a solution is acceptable or not, i.e. there is not two consecutive requests of the instance in the solution.\n",
    "\n",
    "Then, the ```longest_acceptable_duration(instance)``` compute all possible sublists of the instance, test it with the function ```is_acceptable(solution, instance)```, sum every acceptable list (to compute the duration of each acceptable solution) and return the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(solution, instance):\n",
    "    res=True \n",
    "    for x,y in zip(solution[:-1],solution[1:]):\n",
    "        i= instance.index(x)\n",
    "        #index1 = np.where(np.array(instance) == x)\n",
    "        #for i in index[0]:\n",
    "        #if the next element is y \n",
    "        if instance[i+1] == y:\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "def optimal_solution(instance):\n",
    "    sublists=[]\n",
    "    \n",
    "    for i in range(1, len(instance)+1):\n",
    "        sublists+=[list(x) for x in combinations(instance, i)]\n",
    "        \n",
    "    mask = [is_acceptable(solution, instance) for solution in sublists]\n",
    "    acceptable_sol = np.array(sublists)[mask]\n",
    "    \n",
    "    durations = [sum(L) for L in acceptable_sol]\n",
    "    max_duration = max(durations)\n",
    "    \n",
    "    index_optimal_solutions = np.where(np.array(durations)==max_duration)\n",
    "    \n",
    "    return list(np.array(acceptable_sol)[index_optimal_solutions]), max_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40, 50, 20]] 110\n"
     ]
    }
   ],
   "source": [
    "instance = [30, 40, 25, 50, 30, 20]\n",
    "optimal_solutions, max_duration = optimal_solution(instance)\n",
    "print(optimal_solutions, max_duration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adm_hm3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

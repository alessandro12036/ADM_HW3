{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "1vpHmGKKVr3y"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import datetime as dt \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "05q7ZZuI-GDE"
   },
   "outputs": [],
   "source": [
    "def take_n_urls(n):\n",
    "\n",
    "    main_url = \"https://myanimelist.net/topanime.php\"\n",
    "\n",
    "    # this list will contain all the urls we'll retrieve\n",
    "    urls = [] \n",
    "\n",
    "    # each page shows 50 elements and we can retrieve each page by manipulating the \"limit\" query\n",
    "    for limit in range(0, n, 50): \n",
    "        content = requests.get(main_url,\n",
    "                               params={\"limit\": limit})\n",
    "        if content.status_code == 404:\n",
    "            print(f\"Page with limit {limit} was not found. Interrumpting and returning pages found\")\n",
    "            break\n",
    "        soup = bs(content.content, \"html.parser\")\n",
    "\n",
    "        # from the content of each page we retrieve the portions that contain the urls\n",
    "        results = soup.find_all(\"a\", \n",
    "                                class_= \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "\n",
    "        # finally, we take the string containing each url by taking the attribute href,\n",
    "        # and we append them in the urls list\n",
    "        for result in results:\n",
    "            url = result[\"href\"]\n",
    "            if url not in urls:  # check for duplicates\n",
    "                urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdJbPl0CWPPq",
    "outputId": "b98b3450-05af-4830-d7dc-e08e66c56092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urls...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if \"urls.txt\" not in os.listdir():\n",
    "    urls = take_n_urls(20000)\n",
    "    # Since the output of this step has to be a txt file, here we write one with each\n",
    "    # url separated by a newline\n",
    "    with open(\"urls.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(urls_str))\n",
    "else:\n",
    "    with open(\"urls.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        print(\"Loading urls...\")\n",
    "        urls = file.read().split(\"\\n\")\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIe7cbwc_zOt",
    "outputId": "268a24bd-5649-4cce-ff7d-5e9cdf87d09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19130\n",
      "https://myanimelist.net/anime/30839/Big_X_Episode_0\n"
     ]
    }
   ],
   "source": [
    "# we end up with 19131 urls. \n",
    "# I added a check that tells us when we have exceeded the length of the ranking list and returns what has been found\n",
    "# up until that moment (so to avoid losing any more time with get requests that point to nothing)\n",
    "# I know in the assignment they said 20000 but I'm fairly sure that's all the entries. \n",
    "# This is easy to see if we manually set the limit in the url and check the results. \n",
    "# For example: https://myanimelist.net/topanime.php?limit=15000 contains rankings 15001-15050. The first entry is\n",
    "# Big X Episode 0. If we check our list with urls[15000] (remember that our list is 0-indexed) we obtain the same result.\n",
    "# This to me seems to point to a correct behavior from the function, but let me know what you think.\n",
    "\n",
    "print(len(urls)) \n",
    "print(urls[15000])\n",
    "urls_str = list(map(str, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: the index of article start with 0 and not 1 so all ranks are shifted by 1 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the html pages will be stored\n",
    "if \"html_pages\" not in os.listdir():\n",
    "    os.mkdir(\"html_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "t-HVU_RiAfYf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from anime #14550\n"
     ]
    }
   ],
   "source": [
    "if \"counter_pages\" not in os.listdir():\n",
    "    start = 0\n",
    "else:\n",
    "    with open(\"counter_pages\", \"rb\") as counter_file:\n",
    "        start = pickle.load(counter_file) + 1\n",
    "\n",
    "print(f\"Starting from anime #{start}\")\n",
    "n = len(urls)\n",
    "for i in range(start, n):\n",
    "    ranking_page = str(int(np.floor(i/50)))\n",
    "    if i % 50 == 0 or f\"ranking_page_{ranking_page}\" not in os.listdir(\"./html_pages\"):\n",
    "        time.sleep(10)\n",
    "        if \"ranking_page_{ranking_page}\" not in os.listdir(\"./html_pages\"):\n",
    "            os.mkdir(f\"html_pages/ranking_page_{ranking_page}\")\n",
    "    html_page = requests.get(urls[i])\n",
    "    with open (f\"html_pages/ranking_page_{ranking_page}/article_{i}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_page.text)\n",
    "    with open (\"counter_pages\", \"wb\") as counter_file:\n",
    "        pickle.dump(i, counter_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "1. Anime Name (to save as `animeTitle`): String\n",
    "2. Anime Type (to save as `animeType`): String\n",
    "3. Number of episode (to save as `animeNumEpisode`): Integer\n",
    "4. Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "5. Number of members (to save as `animeNumMembers`): Integer\n",
    "6. Score (to save as `animeScore`): Float\n",
    "7. Users (to save as `animeUsers`): Integer\n",
    "8. Rank (to save as `animeRank`): Integer\n",
    "9. Popularity (to save as `animePopularity`): Integer\n",
    "10. Synopsis (to save as `animeDescription`): String\n",
    "11. Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "12. Characters (to save as `animeCharacters`): List of strings.\n",
    "13. Voices (to save as `animeVoices`): List of strings\n",
    "14. Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the tsv files will be stored\n",
    "os.mkdir(\"tsv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_info(num_article, folder='tsv_files'):\n",
    "    ranking_page = str(int(np.floor(num_article/50)))\n",
    "    article=f'html_pages/ranking_page_{ranking_page}/article_{num_article}.html'\n",
    "    with open(article, \"r\", encoding=\"utf-8\") as file:\n",
    "        art= bs(file.read(), 'html.parser')\n",
    "    \n",
    "    #animeTitle\n",
    "    animeTitle = art.find('h1', {'class':\"title-name h1_bold_none\"}).string\n",
    "    #print('animeTitle :',animeTitle)\n",
    "    \n",
    "    \n",
    "    #animeType\n",
    "    animeType = art.find('span', {'class':\"information type\"}).string\n",
    "    #print('animeType :',animeType)\n",
    "    \n",
    "    \n",
    "    #animeNumEpisode and Dates (there is not specific name for those two info)\n",
    "    #list lines with tag <div class=\"spaceit_pad\">\n",
    "    lines = art.find_all('div', {'class':\"spaceit_pad\"})\n",
    "    for line in lines :\n",
    "        #for each div tag there is one span, so here we look for the span tag with 'Episodes:' and 'Aired'\n",
    "        sp= line.find('span', {'class':\"dark_text\"})\n",
    "        # to avoid error if there is no span\n",
    "        if sp is not None :\n",
    "            #for span 'Episodes' (and the div tag which corresponds)\n",
    "            if sp.string == 'Episodes:' :\n",
    "                #extract the content of the right div tag and take the third line which correspond to the number of episodes\n",
    "                animeNumEpisode = int(line.contents[2])\n",
    "                #animeNumEpisode = int(re.findall(r'-?\\d+\\.?\\d*', str(line))[0])           #if we want to use regex  \n",
    "            #for span 'Aired' (and the div tag which corresponds)\n",
    "            if sp.string == 'Aired:' :\n",
    "                str_dates = line.contents[2].split('\\n  ')[1]\n",
    "                #if \"Status: Finished Airing\" (there is a endDate)\n",
    "                if 'to' in str_dates :\n",
    "                    #extract the content of the right div tag and take the third line which correspond to the dates (fix the issue of '\\n')\n",
    "                    str_releaseDate, str_endDate = str_dates.split(' to ')\n",
    "                    #convert into a datetime\n",
    "                    date_format = \"%b %d, %Y\"\n",
    "                    releaseDate, endDate = dt.datetime.strptime(str_releaseDate, date_format), dt.datetime.strptime(str_endDate, date_format)\n",
    "                else :\n",
    "                    str_releaseDate = str_dates\n",
    "                    #convert into a datetime\n",
    "                    date_format = \"%b %d, %Y\"\n",
    "                    releaseDate = dt.datetime.strptime(str_releaseDate, date_format)\n",
    "                    endDate=''\n",
    "    #print('animeNumEpisode :',animeNumEpisode)\n",
    "    #print('releaseDate :',releaseDate)\n",
    "    #print('endDate :',endDate)\n",
    "    \n",
    "    \n",
    "    #animeNumMembers\n",
    "    animeNumMembers = int(art.find('span', {'class':\"numbers members\"}).contents[1].string.replace(',',''))\n",
    "    #print('animeNumMembers :',animeNumMembers)\n",
    "    \n",
    "    \n",
    "    #animeScore\n",
    "    animeScore = float(art.find('div', {'class':\"score-label\"}).string)\n",
    "    #print('animeScore :',animeScore)\n",
    "    \n",
    "    \n",
    "    #animeUsers\n",
    "    animeUsers = int(art.find('span', {'itemprop':\"ratingCount\"}).string)\n",
    "    #print('animeUsers :',animeUsers)\n",
    "    \n",
    "    \n",
    "    #animeRank\n",
    "    animeRank = int(art.find('span', {'class':\"numbers ranked\"}).contents[1].string.replace('#',''))\n",
    "    #print('animeRank :',animeRank)\n",
    "    \n",
    "    \n",
    "    #animePopularity\n",
    "    animePopularity = int(art.find('span', {'class':\"numbers popularity\"}).contents[1].string.replace('#',''))\n",
    "    #print('animePopularity :',animePopularity)\n",
    "    \n",
    "    \n",
    "    #animeDescription\n",
    "    desc = art.find('p', {'itemprop':\"description\"}).contents\n",
    "    animeDescription=''\n",
    "    #remove <br/> Tag and '\\n'\n",
    "    for ele in desc :\n",
    "        #delete tags with regex \n",
    "        ele = re.sub(re.compile('<.*?>'),'', str(ele))\n",
    "        animeDescription += ele\n",
    "    #print('animeDescription :',animeDescription.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "    #animeRelated\n",
    "    animeRelated = []\n",
    "    #store the table which contain related animes\n",
    "    table = art.find('table', {'class':\"anime_detail_related_anime\"})\n",
    "    if table is not None :\n",
    "        #store all links/anime related with 'a' Tag\n",
    "        links = table.find_all('a')\n",
    "        for link in links :\n",
    "            # check if there is a hyperlink and add it in the list if yes \n",
    "            if (link.get('href') is not None) and (link.string is not None):\n",
    "                animeRelated += [link.string]\n",
    "        animeRelated=list(set(animeRelated))\n",
    "    else :\n",
    "        animeRelated=''\n",
    "    #print('animeRelated :',animeRelated)\n",
    "\n",
    "    \n",
    "    #animeCharacters\n",
    "    animeCharacters = art.find_all('h3', {'class':\"h3_characters_voice_actors\"})\n",
    "    animeCharacters = [char.string for char in animeCharacters]\n",
    "    #print('animeCharacters :',animeCharacters)\n",
    "    \n",
    "    \n",
    "    #animeVoices\n",
    "    td_Voices = art.find_all('td', {'class':\"va-t ar pl4 pr4\"})\n",
    "    animeVoices = [voice.find('a').string for voice in td_Voices]\n",
    "    #print('animeVoices :',animeVoices)\n",
    "    \n",
    "    \n",
    "    #animeStaff\n",
    "    #if there is a staff, the div which correspond to the table Staff is the second one (there are div with {'class':\"detail-characters-list clearfix\"})\n",
    "    if len(art.find_all('div', {'class':\"detail-characters-list clearfix\"}))>1 :\n",
    "        div_staff = art.find_all('div', {'class':\"detail-characters-list clearfix\"})[1] \n",
    "        td_staff = div_staff.find_all('td', {'class':\"borderClass\"})\n",
    "        animeStaff=[]\n",
    "        for td in td_staff :\n",
    "            if td.get('width') == None:\n",
    "                animeStaff.append([td.find('a').string,td.find('small').string])\n",
    "    #if there is not staff\n",
    "    else :\n",
    "        animeStaff = ''\n",
    "    #print('animeStaff :',animeStaff)\n",
    "    \n",
    "    #create a .tsv file with attributes\n",
    "    with open(f'{folder}/anime_{num_article}', 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, \\\n",
    "                            animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \\\n",
    "                            animeCharacters, animeVoices, animeStaff])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '?' does not match format '%b %d, %Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-388-4724ccb12d70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcollect_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-387-26c2cbf4f254>\u001b[0m in \u001b[0;36mcollect_info\u001b[1;34m(num_article, folder)\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[1;31m#convert into a datetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[0mdate_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%b %d, %Y\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     \u001b[0mreleaseDate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_releaseDate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_endDate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mstr_releaseDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    576\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[1;32m--> 359\u001b[1;33m                          (data_string, format))\n\u001b[0m\u001b[0;32m    360\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[1;31mValueError\u001b[0m: time data '?' does not match format '%b %d, %Y'"
     ]
    }
   ],
   "source": [
    "for i in range(len(urls)):\n",
    "    collect_info(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a *back-to-back sequence* of requests for appointments. A sequence of requests is of the form\n",
    "    > 30, 40, 25, 50, 30, 20\n",
    "where each number is the time that the person who makes the appointment wants to spend.\n",
    "You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, `[30, 50, 20]` is an acceptable solution (of duration *100*), but `[30, 40, 50, 20]` is not, because *30* and *40* are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration *110*.\n",
    "1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "\n",
    "##### Here we consider that all values in the instance are unique and len(instance) = n\n",
    "\n",
    "To compute the acceptable solution with the longest possible duration, we have to follow several steps :\n",
    "1. Compute every possible solution : So for that, list all sublists which represent each possible list of appointments.\n",
    "2. For each sublist, tell if it is acceptable or not, so if there are two consecutive appointments or not.\n",
    "3. Compute the total duration of each acceptable solution.\n",
    "4. Finally, return the solution which correspond to the maximum duration.\n",
    "\n",
    "```\n",
    "Input: \n",
    "    instance: list of length n\n",
    "\n",
    "function optimal_solution(instance):\n",
    "    n=len(instance)\n",
    "    for i=0 to n: \n",
    "        sublists = sublists + [all sublists with i elements]\n",
    "    \n",
    "    acceptable_solutions=[all element of sublists which are acceptable]\n",
    "    \n",
    "    durations = [duration of each element of acceptable_solutions]\n",
    "    max_duration = max(durations)\n",
    "    optimal_solutions = [sublists of instance with total duration == max_durations]\n",
    "    \n",
    "    return optimal_solutions, max_duration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n",
    "\n",
    "First of all, we create a function ```is_acceptable(solution, instance)``` that says if a solution is acceptable or not, i.e. there is not two consecutive requests of the instance in the solution.\n",
    "\n",
    "Then, the ```longest_acceptable_duration(instance)``` compute all possible sublists of the instance, test it with the function ```is_acceptable(solution, instance)```, sum every acceptable list (to compute the duration of each acceptable solution) and return the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(solution, instance):\n",
    "    res=True \n",
    "    for x,y in zip(solution[:-1],solution[1:]):\n",
    "        i= instance.index(x)\n",
    "        #index1 = np.where(np.array(instance) == x)\n",
    "        #for i in index[0]:\n",
    "        #if the next element is y \n",
    "        if instance[i+1] == y:\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "def optimal_solution(instance):\n",
    "    sublists=[]\n",
    "    \n",
    "    for i in range(1, len(instance)+1):\n",
    "        sublists+=[list(x) for x in combinations(instance, i)]\n",
    "        \n",
    "    mask = [is_acceptable(solution, instance) for solution in sublists]\n",
    "    acceptable_sol = np.array(sublists)[mask]\n",
    "    \n",
    "    durations = [sum(L) for L in acceptable_sol]\n",
    "    max_duration = max(durations)\n",
    "    \n",
    "    index_optimal_solutions = np.where(np.array(durations)==max_duration)\n",
    "    \n",
    "    return list(np.array(acceptable_sol)[index_optimal_solutions]), max_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40, 50, 20]] 110\n"
     ]
    }
   ],
   "source": [
    "instance = [30, 40, 25, 50, 30, 20]\n",
    "optimal_solutions, max_duration = optimal_solution(instance)\n",
    "print(optimal_solutions, max_duration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adm_hm3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

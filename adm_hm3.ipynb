{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1vpHmGKKVr3y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\aurel\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import datetime as dt \n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "### 1.1. Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "05q7ZZuI-GDE"
   },
   "outputs": [],
   "source": [
    "def take_n_urls(n):\n",
    "\n",
    "    main_url = \"https://myanimelist.net/topanime.php\"\n",
    "\n",
    "    # this list will contain all the urls we'll retrieve\n",
    "    urls = [] \n",
    "\n",
    "    # each page shows 50 elements and we can retrieve each page by manipulating the \"limit\" query\n",
    "    for limit in range(0, n, 50): \n",
    "        content = requests.get(main_url,\n",
    "                               params={\"limit\": limit})\n",
    "        if content.status_code == 404:\n",
    "            print(f\"Page with limit {limit} was not found. Interrumpting and returning pages found\")\n",
    "            break\n",
    "        soup = bs(content.content, \"html.parser\")\n",
    "\n",
    "        # from the content of each page we retrieve the portions that contain the urls\n",
    "        results = soup.find_all(\"a\", \n",
    "                                class_= \"hoverinfo_trigger fl-l ml12 mr8\")\n",
    "\n",
    "        # finally, we take the string containing each url by taking the attribute href,\n",
    "        # and we append them in the urls list\n",
    "        for result in results:\n",
    "            url = result[\"href\"]\n",
    "            if url not in urls:  # check for duplicates\n",
    "                urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdJbPl0CWPPq",
    "outputId": "b98b3450-05af-4830-d7dc-e08e66c56092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading urls...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if \"urls.txt\" not in os.listdir():\n",
    "    urls = take_n_urls(20000)\n",
    "    # Since the output of this step has to be a txt file, here we write one with each\n",
    "    # url separated by a newline\n",
    "    with open(\"urls.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(urls_str))\n",
    "else:\n",
    "    with open(\"urls.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        print(\"Loading urls...\")\n",
    "        urls = file.read().split(\"\\n\")\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIe7cbwc_zOt",
    "outputId": "268a24bd-5649-4cce-ff7d-5e9cdf87d09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19130\n",
      "https://myanimelist.net/anime/30839/Big_X_Episode_0\n"
     ]
    }
   ],
   "source": [
    "# we end up with 19131 urls. \n",
    "# I added a check that tells us when we have exceeded the length of the ranking list and returns what has been found\n",
    "# up until that moment (so to avoid losing any more time with get requests that point to nothing)\n",
    "# I know in the assignment they said 20000 but I'm fairly sure that's all the entries. \n",
    "# This is easy to see if we manually set the limit in the url and check the results. \n",
    "# For example: https://myanimelist.net/topanime.php?limit=15000 contains rankings 15001-15050. The first entry is\n",
    "# Big X Episode 0. If we check our list with urls[15000] (remember that our list is 0-indexed) we obtain the same result.\n",
    "# This to me seems to point to a correct behavior from the function, but let me know what you think.\n",
    "\n",
    "print(len(urls)) \n",
    "print(urls[15000])\n",
    "urls_str = list(map(str, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: the index of article start with 0 and not 1 so all ranks are shifted by 1 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the html pages will be stored\n",
    "if \"html_pages\" not in os.listdir():\n",
    "    os.mkdir(\"html_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t-HVU_RiAfYf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_html_pages(urls):\n",
    "    if \"counter_pages\" not in os.listdir():\n",
    "        start = 0\n",
    "    else:\n",
    "        with open(\"counter_pages\", \"rb\") as counter_file:\n",
    "            start = pickle.load(counter_file) + 1\n",
    "\n",
    "    print(f\"Starting from anime #{start}\")\n",
    "    n = len(urls)\n",
    "    for i in range(start, n):\n",
    "        ranking_page = str(int(np.floor(i/50)))\n",
    "        if i % 50 == 0 or f\"ranking_page_{ranking_page}\" not in os.listdir(\"./html_pages\"):\n",
    "            os.mkdir(f\"html_pages/ranking_page_{ranking_page}\")\n",
    "        html_page = requests.get(urls[i])\n",
    "        sleep_timer = 60\n",
    "        while html_page.status_code != 200: # if the status_code is not 200, we've exceeded the number of requests and have to wait\n",
    "            print(f\"Exceeded number of requests while retrieving page #{i}.\\nWaiting {sleep_timer} seconds\")\n",
    "            html_page.close()\n",
    "            time.sleep(sleep_timer)\n",
    "            html_page = requests.get(urls[i])\n",
    "            sleep_timer += 10\n",
    "        with open (f\"html_pages/ranking_page_{ranking_page}/article_{i}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_page.text)\n",
    "        with open (\"counter_pages\", \"wb\") as counter_file:\n",
    "            pickle.dump(i, counter_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from anime #19130\n"
     ]
    }
   ],
   "source": [
    "save_html_pages(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "1. Anime Name (to save as `animeTitle`): String\n",
    "2. Anime Type (to save as `animeType`): String\n",
    "3. Number of episode (to save as `animeNumEpisode`): Integer\n",
    "4. Release and End Dates of anime (to save as `releaseDate` and `endDate`): Convert both release and end date into datetime format.\n",
    "5. Number of members (to save as `animeNumMembers`): Integer\n",
    "6. Score (to save as `animeScore`): Float\n",
    "7. Users (to save as `animeUsers`): Integer\n",
    "8. Rank (to save as `animeRank`): Integer\n",
    "9. Popularity (to save as `animePopularity`): Integer\n",
    "10. Synopsis (to save as `animeDescription`): String\n",
    "11. Related Anime (to save as `animeRelated`): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. List of strings.\n",
    "12. Characters (to save as `animeCharacters`): List of strings.\n",
    "13. Voices (to save as `animeVoices`): List of strings\n",
    "14. Staff (to save as `animeStaff`): Include the staff name and their responsibility/task in a list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ex_aurelie ='C:/Users/aurel/OneDrive/Bureau/IMT/3ème année IMT/0_Cours Sapienza/ADM/Homework/Homework 3'\n",
    "path_ex_alessandro = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the directory where the tsv files will be stored\n",
    "if \"tsv_files\" not in os.listdir():\n",
    "    os.mkdir(\"tsv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_info(num_article, folder='tsv_files'):\n",
    "    ranking_page = str(int(np.floor(num_article/50)))\n",
    "    article=f'{path_ex_aurelie}/html_pages/ranking_page_{ranking_page}/article_{num_article}.html'\n",
    "    with open(article, \"r\", encoding=\"utf-8\") as file:\n",
    "        art= bs(file.read(), 'html.parser')\n",
    "    \n",
    "    #animeTitle\n",
    "    animeTitle = art.find('h1', {'class':\"title-name h1_bold_none\"}).string\n",
    "    #print('animeTitle :',animeTitle)\n",
    "    \n",
    "    \n",
    "    #animeType\n",
    "    animeType = art.find('span', {'class':\"information type\"}).string\n",
    "    #print('animeType :',animeType)\n",
    "    \n",
    "    \n",
    "    #animeNumEpisode and Dates (there is not specific name for those two info)\n",
    "    #list lines with tag <div class=\"spaceit_pad\">\n",
    "    lines = art.find_all('div', {'class':\"spaceit_pad\"})\n",
    "    for line in lines :\n",
    "        #for each div tag there is one span, so here we look for the span tag with 'Episodes:' and 'Aired'\n",
    "        sp= line.find('span', {'class':\"dark_text\"})\n",
    "        # to avoid error if there is no span\n",
    "        if sp is not None :\n",
    "            #for span 'Episodes' (and the div tag which corresponds)\n",
    "            if sp.string == 'Episodes:' :\n",
    "                #extract the content of the right div tag and take the third line which correspond to the number of episodes\n",
    "                if line.contents[2] != '\\n  Unknown\\n  ' :\n",
    "                    animeNumEpisode = int(line.contents[2])\n",
    "                    #animeNumEpisode = int(re.findall(r'-?\\d+\\.?\\d*', str(line))[0])           #if we want to use regex  \n",
    "                else :\n",
    "                    animeNumEpisode = ''\n",
    "            #for span 'Aired' (and the div tag which corresponds)\n",
    "            if sp.string == 'Aired:' :\n",
    "                str_dates = line.contents[2].split('\\n  ')[1]\n",
    "                if str_dates == 'Not available':\n",
    "                    releaseDate = ''\n",
    "                    endDate = ''\n",
    "                else :\n",
    "                    #if \"Status: Finished Airing\" (there is a endDate)\n",
    "                    if ('to' in str_dates) and ('?' not in str_dates):\n",
    "                        #extract the content of the right div tag and take the third line which correspond to the dates (fix the issue of '\\n')\n",
    "                        str_releaseDate, str_endDate = str_dates.split(' to ')\n",
    "\n",
    "                        #choose the right datetime format of str_releaseDate \n",
    "                        if len(str_releaseDate.split(' '))==3:\n",
    "                            date_format_releaseDate = \"%b %d, %Y\"\n",
    "                        elif len(str_releaseDate.split(' '))==2:\n",
    "                            date_format_releaseDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_releaseDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        releaseDate = dt.datetime.strptime(str_releaseDate, date_format_releaseDate)\n",
    "\n",
    "                        #choose the right datetime format of str_endDate \n",
    "                        if len(str_endDate.split(' '))==3:\n",
    "                            date_format_endDate = \"%b %d, %Y\"\n",
    "                        elif len(str_endDate.split(' '))==2:\n",
    "                            date_format_endDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_endDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        endDate = dt.datetime.strptime(str_endDate, date_format_endDate)\n",
    "\n",
    "                    else :\n",
    "                        str_releaseDate = str_dates.split(' to ')[0]\n",
    "                        #choose the right datetime format of str_releaseDate \n",
    "                        if len(str_releaseDate.split(' '))==3:\n",
    "                            date_format_releaseDate = \"%b %d, %Y\"\n",
    "                        elif len(str_releaseDate.split(' '))==2:\n",
    "                            date_format_releaseDate = \"%b %Y\"\n",
    "                        else :\n",
    "                            date_format_releaseDate = \"%Y\"\n",
    "                        #convert str_releaseDate into a datetime\n",
    "                        releaseDate = dt.datetime.strptime(str_releaseDate, date_format_releaseDate)\n",
    "\n",
    "                        endDate=''\n",
    "    #print('animeNumEpisode :',animeNumEpisode)\n",
    "    #print('releaseDate :',releaseDate)\n",
    "    #print('endDate :',endDate)\n",
    "    \n",
    "    \n",
    "    #animeNumMembers\n",
    "    animeNumMembers = int(art.find('span', {'class':\"numbers members\"}).contents[1].string.replace(',',''))\n",
    "    #print('animeNumMembers :',animeNumMembers)\n",
    "    \n",
    "    \n",
    "    #animeScore\n",
    "    score = art.find('div', {'class':\"score-label\"}).string\n",
    "    if score == 'N/A':\n",
    "        animeScore = ''\n",
    "    else :\n",
    "        animeScore = float(score)\n",
    "    #print('animeScore :',animeScore)\n",
    "    \n",
    "    \n",
    "    #animeUsers\n",
    "    if art.find('span', {'itemprop':\"ratingCount\"}) is not None :\n",
    "        animeUsers = int(art.find('span', {'itemprop':\"ratingCount\"}).string)\n",
    "    else :\n",
    "        animeUsers = ''\n",
    "    #print('animeUsers :',animeUsers)\n",
    "    \n",
    "    \n",
    "    #animeRank\n",
    "    if art.find('span', {'class':\"numbers ranked\"}).contents[1].string != 'N/A':\n",
    "        animeRank = int(art.find('span', {'class':\"numbers ranked\"}).contents[1].string.replace('#',''))\n",
    "    else :\n",
    "        animeRank =''\n",
    "    #print('animeRank :',animeRank)\n",
    "    \n",
    "    \n",
    "    #animePopularity\n",
    "    animePopularity = int(art.find('span', {'class':\"numbers popularity\"}).contents[1].string.replace('#',''))\n",
    "    #print('animePopularity :',animePopularity)\n",
    "    \n",
    "    \n",
    "    #animeDescription\n",
    "    desc = art.find('p', {'itemprop':\"description\"}).contents\n",
    "    animeDescription=''\n",
    "    #remove <br/> Tag and '\\n'\n",
    "    for ele in desc :\n",
    "        #delete tags with regex \n",
    "        ele = re.sub(re.compile('<.*?>'),'', str(ele))\n",
    "        animeDescription += ele\n",
    "        animeDescription = animeDescription.replace('\\n','')\n",
    "    #print('animeDescription :',animeDescription.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "    #animeRelated\n",
    "    animeRelated = []\n",
    "    #store the table which contain related animes\n",
    "    table = art.find('table', {'class':\"anime_detail_related_anime\"})\n",
    "    if table is not None :\n",
    "        #store all links/anime related with 'a' Tag\n",
    "        links = table.find_all('a')\n",
    "        for link in links :\n",
    "            # check if there is a hyperlink and add it in the list if yes \n",
    "            if (link.get('href') is not None) and (link.string is not None):\n",
    "                animeRelated += [link.string]\n",
    "        animeRelated=list(set(animeRelated))\n",
    "    else :\n",
    "        animeRelated=''\n",
    "    #print('animeRelated :',animeRelated)\n",
    "\n",
    "    \n",
    "    #animeCharacters\n",
    "    animeCharacters = art.find_all('h3', {'class':\"h3_characters_voice_actors\"})\n",
    "    animeCharacters = [char.string for char in animeCharacters]\n",
    "    #print('animeCharacters :',animeCharacters)\n",
    "    \n",
    "    \n",
    "    #animeVoices\n",
    "    td_Voices = art.find_all('td', {'class':\"va-t ar pl4 pr4\"})\n",
    "    animeVoices = [voice.find('a').string for voice in td_Voices]\n",
    "    #print('animeVoices :',animeVoices)\n",
    "    \n",
    "    \n",
    "    #animeStaff\n",
    "    #if there is a staff, the div which correspond to the table Staff is the second one (there are div with {'class':\"detail-characters-list clearfix\"})\n",
    "    if len(art.find_all('div', {'class':\"detail-characters-list clearfix\"}))>1 :\n",
    "        div_staff = art.find_all('div', {'class':\"detail-characters-list clearfix\"})[1] \n",
    "        td_staff = div_staff.find_all('td', {'class':\"borderClass\"})\n",
    "        animeStaff=[]\n",
    "        for td in td_staff :\n",
    "            if td.get('width') == None:\n",
    "                animeStaff.append([td.find('a').string,td.find('small').string])\n",
    "    #if there is not staff\n",
    "    else :\n",
    "        animeStaff = ''\n",
    "    #print('animeStaff :',animeStaff)\n",
    "    \n",
    "    #create a .tsv file with attributes\n",
    "    with open(f'{folder}/anime_{num_article}', 'wt', encoding=\"utf8\") as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, \\\n",
    "                            animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, \\\n",
    "                            animeCharacters, animeVoices, animeStaff])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(17346, len(urls)):\n",
    "    collect_info(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Pre-processing\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stemming\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "\n",
    "# For identifying the stop words\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    try:\n",
    "        tokenized = nltk.word_tokenize(text)\n",
    "        stemmed = [porter.stem(word) for word in tokenized if ((word.lower() not in eng_stopwords) and (word not in string.punctuation))]\n",
    "    except TypeError as e:\n",
    "        print(text)\n",
    "        raise TypeError\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# To sort the files correctly\n",
    "def alphanumeric_sort(key):\n",
    "    num = int(re.search(\"([0-9]+)\", key).group(0))\n",
    "    return num\n",
    "\n",
    "def merge_tsvs(path, colnames):\n",
    "    files = sorted(os.listdir(path), key=alphanumeric_sort)\n",
    "    df = pd.read_csv(path+files[0],\n",
    "                     names=colnames,\n",
    "                     sep=\"\\t\", engine='python')\n",
    "    for file_name in files[1:]:\n",
    "        df2 = pd.read_csv(path+file_name,\n",
    "                          names=colnames,\n",
    "                          sep=\"\\t\", engine='python')\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the implemented sorting algorithm\n",
    "#print(sorted(os.listdir(\"./tsv_files/\"), key=alphanumeric_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./tsv_files/\"\n",
    "colnames = [\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\", \"animeNumMembers\",\n",
    "            \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\", \"animeDescription\", \"animeRelated\",\n",
    "            \"animeCharacters\", \"animeVoices\", \"animeStaff\"]\n",
    "df = merge_tsvs(path, colnames)\n",
    "\n",
    "# Save our df in csv format\n",
    "df.to_csv(\"./html_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of our dataframe with an extra column containing the preprocessed synopsis\n",
    "df_new = df.assign(tokenized_desc=df[\"animeDescription\"].apply(lambda x: process_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19130 entries, 0 to 19129\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   animeTitle        19130 non-null  object \n",
      " 1   animeType         19130 non-null  object \n",
      " 2   animeNumEpisode   18620 non-null  float64\n",
      " 3   releaseDate       18760 non-null  object \n",
      " 4   endDate           8451 non-null   object \n",
      " 5   animeNumMembers   19130 non-null  int64  \n",
      " 6   animeScore        13436 non-null  float64\n",
      " 7   animeUsers        13436 non-null  float64\n",
      " 8   animeRank         17307 non-null  float64\n",
      " 9   animePopularity   19130 non-null  int64  \n",
      " 10  animeDescription  19130 non-null  object \n",
      " 11  animeRelated      12706 non-null  object \n",
      " 12  animeCharacters   19130 non-null  object \n",
      " 13  animeVoices       19130 non-null  object \n",
      " 14  animeStaff        10247 non-null  object \n",
      " 15  tokenized_desc    19130 non-null  object \n",
      "dtypes: float64(4), int64(2), object(10)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df_with_tokens.p\", \"wb\") as file:\n",
    "    pickle.dump(df_new, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "#### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First i chose to create the vocabulary file as a DataFrame store as a .csv file \n",
    "def create_vocabulary(corpus, name_voc_file = \"vocabulary.pkl\"):\n",
    "    voc = set()\n",
    "    i=0\n",
    "    for doc in corpus :\n",
    "        #print(i)\n",
    "        #i+=1\n",
    "        voc = voc.union(set(doc))\n",
    "\n",
    "    dict_voc = dict(zip(sorted(voc),range(len(voc))))\n",
    "    with open(name_voc_file, \"wb\") as file:\n",
    "        pickle.dump(dict_voc, file)\n",
    "    return dict_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(corpus,voc, name_inv_ind_file = \"inverted_index.pkl\"):\n",
    "    #create a inverted_index \"empty\", i.e. only with term_id of vocabulary\n",
    "    inverted_index = dict()\n",
    "    for term, term_id in voc.items() :\n",
    "        inverted_index[term_id]=set()\n",
    "\n",
    "    for doc, num_doc in zip(corpus,range(len(corpus))) :\n",
    "        #print(num_doc)\n",
    "        words_checked = []\n",
    "        for word in doc :\n",
    "            if word not in words_checked: # to avoid looking for the same word more than once in the same doc\n",
    "                words_checked.append(word)\n",
    "                term_id = voc[word]\n",
    "                inverted_index[term_id]=inverted_index[term_id].union(set([num_doc]))\n",
    "                \n",
    "    for term_id, docs in inverted_index.items() :\n",
    "        inverted_index[term_id]=sorted(list(inverted_index[term_id]))\n",
    "    \n",
    "    #save the inverted_index as a .pkl file\n",
    "    with open(name_inv_ind_file, \"wb\") as file:\n",
    "        pickle.dump(inverted_index,file)\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [0, 1], 2: [0], 3: [1], 4: [0]}\n"
     ]
    }
   ],
   "source": [
    "#test with a simple case\n",
    "\n",
    "L=['A','C', 'B', 'A', 'E']\n",
    "corpus_test=[L]+[['B','D']]\n",
    "create_vocabulary(corpus_test, \"vocabulary_test.pkl\")\n",
    "\n",
    "with open(\"vocabulary_test.pkl\", \"rb\") as file:\n",
    "    voc_test = pickle.load(file)\n",
    "        \n",
    "inverted_index(corpus_test,voc_test, \"inverted_index_test.pkl\" )\n",
    "\n",
    "with open(\"inverted_index_test.pkl\", \"rb\") as file:\n",
    "    inv_ind_test = pickle.load(file)\n",
    "print(inv_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset of synopsis stored in df_with_tokens.p\n",
    "def download_corpus(name_file_corpus = 'df_with_tokens.p'):\n",
    "    print('Downloading corpus... ', end ='')\n",
    "    with open(name_file_corpus, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "\n",
    "    corpus = list(df['tokenized_desc'])\n",
    "    print('Done')\n",
    "    return corpus\n",
    "\n",
    "#download the voc or create it if it does not already exist\n",
    "def download_voc(corpus, name_voc_file):\n",
    "    print('Downloading vocabulary... ', end ='')\n",
    "    if name_voc_file not in os.listdir():\n",
    "        voc = create_vocabulary(corpus, name_voc_file)\n",
    "    else :\n",
    "        with open(name_voc_file, \"rb\") as file:\n",
    "            voc = pickle.load(file)\n",
    "    print('Done')\n",
    "    return voc\n",
    "\n",
    "#download the inverted index or create it if it does not already exist\n",
    "def download_inverted_index(corpus,voc, name_inv_ind_file):\n",
    "    print('Downloading inverted index... ', end ='')\n",
    "    if name_inv_ind_file not in os.listdir():\n",
    "        inv_ind = inverted_index(corpus,voc, name_inv_ind_file)\n",
    "    else :\n",
    "        with open(name_inv_ind_file, \"rb\") as file:\n",
    "            inv_ind = pickle.load(file)\n",
    "    print('Done')\n",
    "    return inv_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus... Done\n",
      "Downloading vocabulary... Done\n",
      "Downloading inverted index... Done\n"
     ]
    }
   ],
   "source": [
    "#define the names of the interesting files \n",
    "name_voc_file = \"vocabulary.pkl\"\n",
    "name_file_corpus = 'df_with_tokens.p'\n",
    "name_inv_ind_file = \"inverted_index.pkl\"\n",
    "\n",
    "#download the corpus and the vocabulary\n",
    "corpus = download_corpus(name_file_corpus)\n",
    "voc = download_voc(corpus, name_voc_file)\n",
    "inv_ind = download_inverted_index(corpus,voc, name_inv_ind_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "```saiyan race```\n",
    "\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `animeTitle`\n",
    "* `animeDescription`\n",
    "* `Url`\n",
    "\n",
    "__Example Output__:\n",
    "\n",
    "| animeTitle | animeDescription | Url |\n",
    "|:-----------------------------:|:-----:|:------------------------------------------------------------:|\n",
    "| Fullmetal Alchemist: Brotherhood | ... | https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood |\n",
    "| Gintama | ... | https://myanimelist.net/anime/28977/Gintama%C2%B0 |\n",
    "| Shingeki no Kyojin Season 3 Part 2 | ... | https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2 |\n",
    "\n",
    "If everything works well in this step, you can go to the next point, and make your Search Engine more complex and better in answering queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(voc, inverted_index):\n",
    "    #ask the query to the user\n",
    "    query = input('What is your query ?')\n",
    "    \n",
    "    # apply the preprocessing to the query\n",
    "    query = process_text(query.lower())\n",
    "    \n",
    "    # initialization with the set of the document which the first word of the query\n",
    "    first_word = query[0]\n",
    "    # check if first_word is in the vocabulary (otherwise, is doesn't exist any document to answer to the query)\n",
    "    if first_word in voc :\n",
    "        first_term_id = voc[first_word]\n",
    "        docs_list = set(inverted_index[first_term_id])\n",
    "        \n",
    "        for word in query[1:] :\n",
    "            #if the next word is in the voc, it means that it exists documents with this word\n",
    "            if word in voc :\n",
    "                # store the term_id and find the documents which contain this word in the inverted_index\n",
    "                term_id = voc[word]\n",
    "                docs = inverted_index[term_id]\n",
    "                \n",
    "                # compute the intersection because every word of the query has to be in the description of the doc \n",
    "                docs_list = docs_list.intersection(set(inverted_index[first_term_id]))\n",
    "                \n",
    "                # if the intersection between the previous docs_list and the set of document with the next word,\\\n",
    "                # no document answers to the query\n",
    "                if len(docs_list) == 0 : \n",
    "                    print('Nothing correspond to your queries')\n",
    "                    return\n",
    "                \n",
    "            else : # no document answers to the query\n",
    "                print('Nothing correspond to your queries')\n",
    "                return\n",
    "        \n",
    "        #Now we have the doc IDs so we can merge interesting information \n",
    "        html_df = pd.read_csv(path_ex_aurelie+\"/html_df.csv\") #csv which contains tsv line of each document\n",
    "        cols = [\"animeTitle\", \"animeDescription\"]\n",
    "        result = html_df.iloc[sorted(list(docs_list))][cols]\n",
    "        result['Url'] = [urls[i] for i in sorted(list(docs_list))]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    else : # no document answers to the query\n",
    "        print('Nothing correspond to your queries')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your query ?saiyan race\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>Dragon Ball Super</td>\n",
       "      <td>Seven years after the events of Dragon Ball Z,...</td>\n",
       "      <td>https://myanimelist.net/anime/30694/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>Dragon Ball Z Movie 14: Kami to Kami</td>\n",
       "      <td>Following the defeat of a great adversary, Gok...</td>\n",
       "      <td>https://myanimelist.net/anime/14837/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...</td>\n",
       "      <td>As Goku investigates the destruction of the So...</td>\n",
       "      <td>https://myanimelist.net/anime/901/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>Dragon Ball: Ossu! Kaettekita Son Gokuu to Nak...</td>\n",
       "      <td>Based on an original concept by the original a...</td>\n",
       "      <td>https://myanimelist.net/anime/5152/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>Dragon Ball Z Movie 10: Kiken na Futari! Super...</td>\n",
       "      <td>After his loss to Goku, Broly crash lands and ...</td>\n",
       "      <td>https://myanimelist.net/anime/903/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>Dragon Ball Z: Summer Vacation Special</td>\n",
       "      <td>One peaceful afternoon, the Son family and fri...</td>\n",
       "      <td>https://myanimelist.net/anime/22695/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>Dragon Ball Z: Zenbu Misemasu Toshi Wasure Dra...</td>\n",
       "      <td>In this film, which is believed to take place ...</td>\n",
       "      <td>https://myanimelist.net/anime/22699/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6560</th>\n",
       "      <td>Galo Sengen</td>\n",
       "      <td>Galo Sengen (\"Galish Man GAL\" in English / GAL...</td>\n",
       "      <td>https://myanimelist.net/anime/20929/Galo_Sengen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>Dragon Ball Z: The Real 4-D at Super Tenkaichi...</td>\n",
       "      <td>Dragon Ball Z: The Real 4-D at Super Tenkaichi...</td>\n",
       "      <td>https://myanimelist.net/anime/42449/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9229</th>\n",
       "      <td>Dragon Ball Z Movie 11: Super Senshi Gekiha!! ...</td>\n",
       "      <td>Jaga Bada, Mr. Satan's old sparring partner, h...</td>\n",
       "      <td>https://myanimelist.net/anime/904/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "364                                       Dragon Ball Z   \n",
       "401                            Dragon Ball Super: Broly   \n",
       "1035                                    Dragon Ball Kai   \n",
       "1467  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1961                                  Dragon Ball Super   \n",
       "2015               Dragon Ball Z Movie 14: Kami to Kami   \n",
       "2296  Dragon Ball Z Movie 08: Moetsukiro!! Nessen, R...   \n",
       "4339  Dragon Ball: Ossu! Kaettekita Son Gokuu to Nak...   \n",
       "4673  Dragon Ball Z Movie 10: Kiken na Futari! Super...   \n",
       "5664             Dragon Ball Z: Summer Vacation Special   \n",
       "6092  Dragon Ball Z: Zenbu Misemasu Toshi Wasure Dra...   \n",
       "6560                                        Galo Sengen   \n",
       "8969  Dragon Ball Z: The Real 4-D at Super Tenkaichi...   \n",
       "9229  Dragon Ball Z Movie 11: Super Senshi Gekiha!! ...   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "364   Five years after winning the World Martial Art...   \n",
       "401   Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1035  Five years after the events of Dragon Ball, ma...   \n",
       "1467  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1961  Seven years after the events of Dragon Ball Z,...   \n",
       "2015  Following the defeat of a great adversary, Gok...   \n",
       "2296  As Goku investigates the destruction of the So...   \n",
       "4339  Based on an original concept by the original a...   \n",
       "4673  After his loss to Goku, Broly crash lands and ...   \n",
       "5664  One peaceful afternoon, the Son family and fri...   \n",
       "6092  In this film, which is believed to take place ...   \n",
       "6560  Galo Sengen (\"Galish Man GAL\" in English / GAL...   \n",
       "8969  Dragon Ball Z: The Real 4-D at Super Tenkaichi...   \n",
       "9229  Jaga Bada, Mr. Satan's old sparring partner, h...   \n",
       "\n",
       "                                                    Url  \n",
       "364     https://myanimelist.net/anime/813/Dragon_Ball_Z  \n",
       "401   https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "1035  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "1467  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "1961  https://myanimelist.net/anime/30694/Dragon_Bal...  \n",
       "2015  https://myanimelist.net/anime/14837/Dragon_Bal...  \n",
       "2296  https://myanimelist.net/anime/901/Dragon_Ball_...  \n",
       "4339  https://myanimelist.net/anime/5152/Dragon_Ball...  \n",
       "4673  https://myanimelist.net/anime/903/Dragon_Ball_...  \n",
       "5664  https://myanimelist.net/anime/22695/Dragon_Bal...  \n",
       "6092  https://myanimelist.net/anime/22699/Dragon_Bal...  \n",
       "6560    https://myanimelist.net/anime/20929/Galo_Sengen  \n",
       "8969  https://myanimelist.net/anime/42449/Dragon_Bal...  \n",
       "9229  https://myanimelist.net/anime/904/Dragon_Ball_...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_1(voc, inv_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(word, doc, corpus, idf=None):\n",
    "    tf = doc.count(word) / len(doc)\n",
    "    counter_docs = 0\n",
    "    # if the idf parameter has not been provided, we compute it\n",
    "    if idf == None:\n",
    "        for text in corpus:\n",
    "            if word in text:\n",
    "                counter_docs += 1\n",
    "        idf = np.log(len(corpus) / counter_docs)\n",
    "    tfidf = tf * idf\n",
    "    return idf, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_inverted_index(corpus, voc, name_inv_ind_tf_idf_file = \"inverted_index_2.p\", name_idfs_file = \"idfs.p\"):\n",
    "    inverted_index_2 = dict()\n",
    "    # first, we initialize each field in the inverted_index\n",
    "    for term_id in voc.values() :\n",
    "        inverted_index_2[term_id]=list()\n",
    "        \n",
    "    idf_calculated = {} # the idfs can be calculated once for each word since idf = np.log(len(corpus) / documents_with_word)\n",
    "    \n",
    "    for doc, num_doc in zip(corpus,range(len(corpus))) :\n",
    "        words_checked = []\n",
    "        for word in doc:\n",
    "            if word not in words_checked: # to avoid looking for the same word more than once in the same doc\n",
    "                term_id = voc[word]\n",
    "                # if this is the first time we encounter this word, we need to obtain the idf and save it for future use\n",
    "                if word not in idf_calculated.keys():\n",
    "                    idf, tfidf = get_tfidf(word, doc, corpus)\n",
    "                    idf_calculated[word] = idf\n",
    "                # otherwise, we provide it to the function directly\n",
    "                else:\n",
    "                    _, tfidf = get_tfidf(word, doc, corpus, idf)\n",
    "                # we add the doc index and the tfidf score to the dictionary\n",
    "                inverted_index_2[term_id].append([num_doc, tfidf])\n",
    "                # we mark this word as \"checked\" for this document\n",
    "                words_checked.append(word)\n",
    "    \n",
    "    # we order the items by tfidf score for that term\n",
    "    for term_id, docs in inverted_index_2.items() :\n",
    "        inverted_index_2[term_id]=sorted(inverted_index_2[term_id], key=lambda x: x[1])\n",
    "    \n",
    "    # finally we save the item in order to avoid having to create the index again\n",
    "    with open (name_inv_ind_tf_idf_file, \"wb\") as file:\n",
    "        pickle.dump(inverted_index_2, file)\n",
    "    # and also the idfs array\n",
    "    with open (name_idfs_file, \"wb\") as file:\n",
    "        pickle.dump(idf_calculated, file)\n",
    "        \n",
    "    return inverted_index_2, idf_calculated # we also return the calculated idfs so to avoid calculating them over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the inverted_index_tfidf or create it if it does not already exist\n",
    "def download_inverted_index_tfidf(corpus,voc, name_inv_ind_tf_idf_file, name_idfs_file):\n",
    "    print('Downloading inverted index tf.idf... ', end ='')\n",
    "    if (name_inv_ind_tf_idf_file not in os.listdir()) or (name_idfs_file not in os.listdir()):\n",
    "        ii_2, idfs = second_inverted_index(corpus,voc, name_inv_ind_tf_idf_file, name_idfs_file)\n",
    "    else :\n",
    "        with open(name_inv_ind_tf_idf_file, \"rb\") as file:\n",
    "            ii_2 = pickle.load(file)\n",
    "        with open(name_idfs_file, \"rb\") as file:\n",
    "            idfs = pickle.load(file)\n",
    "    print('Done')\n",
    "    return ii_2, idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading inverted index tf.idf... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-9f70e3d2a6fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mii_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_inverted_index_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inverted_index_2.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"idfs.p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-338400a3e9a5>\u001b[0m in \u001b[0;36mdownload_inverted_index_tfidf\u001b[1;34m(corpus, voc, name_inv_ind_tf_idf_file, name_idfs_file)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading inverted index tf.idf... '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_inv_ind_tf_idf_file\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_idfs_file\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mii_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msecond_inverted_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_inv_ind_tf_idf_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_idfs_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_inv_ind_tf_idf_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-cef470ba71b3>\u001b[0m in \u001b[0;36msecond_inverted_index\u001b[1;34m(corpus, voc, name_inv_ind_tf_idf_file, name_idfs_file)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;31m# if this is the first time we encounter this word, we need to obtain the idf and save it for future use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midf_calculated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                     \u001b[0midf_calculated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;31m# otherwise, we provide it to the function directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-caa885bade31>\u001b[0m in \u001b[0;36mget_tfidf\u001b[1;34m(word, doc, corpus, idf)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mcounter_docs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0midf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcounter_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ii_2, idfs = download_inverted_index_tfidf(corpus,voc, \"inverted_index_2.p\", \"idfs.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the Cosine Similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    num = np.dot(vec1, vec2)\n",
    "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    cos = num / denom\n",
    "    return cos\n",
    "\n",
    "# this is good for binary variables\n",
    "def tanimoto_distance(vec1, vec2):\n",
    "    num = np.dot(vec1, vec2)\n",
    "    denom = np.square(np.linalg.norm(vec1)) + np.square(np.linalg.norm(vec2)) - num\n",
    "    tanimoto = num / denom\n",
    "    return tanimoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# implement the case in which the query is not in any document\n",
    "\n",
    "def search_k_matches(query, corpus, voc, ii, idfs, k=10):\n",
    "    \n",
    "    #store the file with all information about the set of html pages (use at the end to return information of relevant documents)\n",
    "    df = pd.read_csv(\"./html_df.csv\")\n",
    "    \n",
    "    # apply preprocessing the query\n",
    "    query = process_text(query.lower())\n",
    "    \n",
    "    # initialize the dictionary of the result\n",
    "    dict_relevant = {}\n",
    "    \n",
    "    for word in query:\n",
    "        if word in voc.keys(): # checks if query is in our vocabulary\n",
    "            term_id = voc[word]\n",
    "            #find documents with non_zero tf_idf with the \"word\"\n",
    "            for doc_info in ii[term_id]:\n",
    "                # doc_info[0] is the document_id (doc_info[1] is the associated tf-idf )\n",
    "                # if it is not already in the dict_relevant, add it as key\n",
    "                if doc_info[0] not in dict_relevant.keys():\n",
    "                    dict_relevant[doc_info[0]] = []\n",
    "                #add the associated tf-idf (term-document)\n",
    "                dict_relevant[doc_info[0]].append(doc_info[1])\n",
    "    \n",
    "    len_query = len(query)\n",
    "    #if a word of query is not in idfs.keys(), it means that idf(x) = 0, so no need to keep it in our vector\n",
    "    query_vector = [(query.count(x) / len_query) * idfs[x] for x in query if x in idfs.keys()] # we treat the query as a document\n",
    "    distances = []\n",
    "    for key in dict_relevant.keys():\n",
    "        vector = dict_relevant[key]\n",
    "        if len(vector) == len(query_vector): # this assures the conjuctive (and) property of the search engine (i.e. documents description contains every word of query)\n",
    "            distances.append((-cosine_similarity(query_vector, vector), key)) # negative of cosine_similarity to get max heap\n",
    "    \n",
    "    # convert \"distances\" as a heap\n",
    "    heapq.heapify(distances)\n",
    "    n_matches = len(distances)\n",
    "    final_results = []\n",
    "    # deal with the case where k > n_matches\n",
    "    for i in range(min(k, n_matches)):\n",
    "        # return and store [document_id, cosine_similarity] of the i-th best document according to the query in \"final_results\"\n",
    "        el = heapq.heappop(distances)\n",
    "        final_results.append([el[1], -el[0]]) # make the cosine distance positive again for the output\n",
    "    \n",
    "    #print(final_results)\n",
    "    indices = [x[0] for x in final_results]\n",
    "    distances = [x[1] for x in final_results]\n",
    "    cols = [\"animeTitle\", \"animeDescription\"]\n",
    "    # keep only the relevant lines\n",
    "    partial_df = df.iloc[indices][cols]\n",
    "    # add two columns : \"Url\" and \"Similarity\"\n",
    "    final_df = partial_df.assign(Url=[urls[i] for i in indices],\n",
    "                                 Similarity=distances)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test it against the keywords \"ranma urusei\" which are two characters from two different animes that only meet in a special crossover episode called \"It's a Rumic World\". We can see that the search engine correctly gives this result as the best match, followed by another Ranma 1/2 episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-5372d29b96c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ranma urusei\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_k_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mii_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'idfs' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"ranma urusei\"\n",
    "output = search_k_matches(query, corpus, voc, ii_2, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-61353e8b2d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Currently not working, cosine similarity doesn't behave well with 0s, will try to think of a solution tomorrow morning***\n",
    "\n",
    "***Update***\n",
    "https://www.sciencedirect.com/topics/computer-science/cosine-similarity\n",
    "found here that for binary features, the tanimoto distance may be a good choice. So tanimoto distance for the parameterized queries (like year, genre and so on) and cosine distance for the tfidf and then we need to find a way to combine them in a unique score. Maybe average them out?\n",
    "\n",
    "***Update 2***\n",
    "Seems to be working now, I prepared a couple of experiments in the cell below to test it\n",
    "Basically we use the tanimoto distance for the parameters (that are binary) and cosine distance for the tfidf. Then we average the two scores. \n",
    "<br>\n",
    "The cosine should weight more, and each parameters of the tanimoto should weigh 1/5, so if the main query points toward an anime the score won't probably be overturned just by one parameter matching, which I believe is what we want.\n",
    "\n",
    "<br>\n",
    "----------------\n",
    "<br>\n",
    "The idea here is to obtain a query with several parameters, like for example year of release, voice actors, genre and so on. This query can be obtained either directly in the format \"query [parameter1=x, parameter2=y...]\" or through a form that assembles the same string so that the inner logic of the algorithm doesn't change either way.<br><br>\n",
    "The function process_query then builds a dictionary for the query by extrapolating the parameters through regular expressions and returns this dictionary to the main function.<br><br>\n",
    "Now, in order to come up with some score for the matches in this parameters (which are binary, either 1 or 0 depending on whether they match or not), we use the tanimoto distance and then we average it with the cosine distance between the tfidfs. Meaning that if in the previous search engine (the one with the tfidf) we had for each doc a vector of tfidfs, now we have the same thing but also an additional vector of ones or zeros on which we compute the tanimoto.\n",
    "As always, we treat the query as its own document with its own tf-idf scores and perfect parameters matches with itself (so its parameters are all 1s).\n",
    "\n",
    "It's very rough, and it just allows for a binary definition of the parameters (either they match or they don't). I know that for certain parameters we could probably achieve a more refined scoring, but it seemed like a lot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thinking of a query like \"sayian race [year=1999 episodes=22]\". So first main query and then in squared parentheses\n",
    "# the parameters.\n",
    "\n",
    "# processes the query in its single components\n",
    "def process_query(query):\n",
    "    query_dict = dict()\n",
    "    main_query = re.search(\"^(.+)\\[\", query)\n",
    "    anime_voices = re.search(\"voices=\\(([^\\)]+)\\)\", query)\n",
    "    anime_chars = re.search(\"characters=\\(([^\\)]+)\\)\", query)\n",
    "    anime_related = re.search(\"related=\\(([^\\)]+)\\)\", query)\n",
    "    year = re.search(\"year=([0-9]+)[,\\]]\", query)\n",
    "    anime_type = re.search(\"type=([a-zA-Z]+)[,\\]]\", query)\n",
    "    \n",
    "    if year:\n",
    "        query_dict[\"release_year\"] = year.groups()[0]\n",
    "    if anime_type:\n",
    "        query_dict[\"anime_type\"] = anime_type.groups()[0]\n",
    "    # transform these fields in lists before putting them in the dictionary\n",
    "    if anime_voices:\n",
    "        anime_voices = anime_voices.groups()[0]\n",
    "        query_dict[\"anime_voices\"] =  anime_voices.strip().split(\",\")\n",
    "\n",
    "    if anime_chars:\n",
    "        anime_chars = anime_chars.groups()[0]\n",
    "        query_dict[\"anime_characters\"] = anime_chars.strip().split(\",\")\n",
    "    if anime_related:\n",
    "        anime_related = anime_related.groups()[0]\n",
    "        query_dict[\"anime_related\"] = anime_related.strip().split(\",\")\n",
    "    \n",
    "    # preprocesses main query before putting it in the dictionary\n",
    "    query_dict[\"main_text_query\"] = process_text(main_query.groups()[0])\n",
    "    #print(query_dict)\n",
    "    \n",
    "    return query_dict\n",
    "    \n",
    "\n",
    "# we need to treat the main query as it was its own document (I found references online on this).\n",
    "# This is the same thing i implemented in 2.2.2, I just put it in a function here.\n",
    "def evaluate_main_query(query, corpus, voc, ii, idfs):\n",
    "    dict_relevant = {}\n",
    "    for word in query:\n",
    "        if word in voc.keys(): # checks if query is in our vocabulary\n",
    "            term_id = voc[word]\n",
    "            for doc_info in ii[term_id]:\n",
    "                if doc_info[0] not in dict_relevant.keys():\n",
    "                    dict_relevant[doc_info[0]] = []\n",
    "                dict_relevant[doc_info[0]].append(doc_info[1])\n",
    "                \n",
    "    return dict_relevant\n",
    "    \n",
    "    \n",
    "# Here we return a vector of binary values based on the parameterized queries\n",
    "def evaluate_parameters(query_d, df, anime_num):\n",
    "    relevant_row = df.iloc[anime_num]\n",
    "    vector = []\n",
    "    for dict_key in sorted(query_d.keys()):\n",
    "        if dict_key == \"release_year\":\n",
    "            year = datetime.datetime.strptime(relevant_row[\"releaseDate\"], \"%Y-%m-%d %H:%M:%S\").year\n",
    "            vector.append(int(str(year) == query_d[dict_key])) # evaluates the boolean to an integer\n",
    "            \n",
    "        elif dict_key == \"anime_type\":\n",
    "            anime_type = relevant_row[\"animeType\"]\n",
    "            score = int(anime_type.lower() == query_d[dict_key].lower())\n",
    "            vector.append(score)\n",
    "            \n",
    "        elif dict_key == \"anime_characters\":\n",
    "            chars = relevant_row[\"animeCharacters\"].lower()\n",
    "            matches = 0\n",
    "            for char_query in query_d[dict_key]:\n",
    "                char_query = char_query.lower()\n",
    "                if char_query in chars:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "            \n",
    "        elif dict_key == \"anime_related\":\n",
    "            related = relevant_row[\"animeRelated\"].lower()\n",
    "            matches = 0\n",
    "            for rel_anime_query in query_d[dict_key]:\n",
    "                rel_anime_query = rel_anime_query.lower()\n",
    "                if rel_anime_query in related:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "            \n",
    "        elif dict_key == \"anime_voices\":\n",
    "            voices = relevant_row[\"animeVoices\"].lower()\n",
    "            matches = 0\n",
    "            for voices_query in query_d[dict_key]:\n",
    "                voices_query = voices_query.lower()\n",
    "                if voices_query in voices:\n",
    "                    matches += 1\n",
    "            score = matches / len(query_d[dict_key])\n",
    "            vector.append(score)\n",
    "        \n",
    "    return vector\n",
    "\n",
    "# Here we obtain the query via form\n",
    "def get_query_with_form():\n",
    "    query_d = dict()\n",
    "    main_query = input(\"Enter your query: \")\n",
    "    year = input(\"Year it was released: \")\n",
    "    anime_type = input(\"Type of anime: \")\n",
    "    voices = input(\"Voice actors: \")\n",
    "    characters = input(\"Characters: \")\n",
    "    related = input(\"Related animes: \")\n",
    "    \n",
    "    query_string = (f\"{main_query} [year={year}, anime_type={anime_type}, related=({related}),\"\n",
    "                    f\"voices=({voices}), characters=({characters})]\")\n",
    "        \n",
    "    return query_string\n",
    "\n",
    "\n",
    "def search_k_matches_2(corpus, voc, ii, idfs, query=None, k=10):\n",
    "    df = pd.read_csv(\"./html_df.csv\")\n",
    "    if not query:\n",
    "        query = get_query_with_form()\n",
    "    query_dict = process_query(query)\n",
    "    # here we process the main query as it was its own document with perfect match parameters.\n",
    "    main_query = query_dict[\"main_text_query\"]\n",
    "    len_query = len(main_query)\n",
    "    query_main_vector = [(main_query.count(x) / len_query) * idfs[x] for x in main_query if x in idfs.keys()]\n",
    "    query_parameters_vector = [1]*(len(query_dict.keys())-1)\n",
    "    \n",
    "    # initialize the distances array that will contain all our distances and extract the synopsis\n",
    "    # that match the main query\n",
    "    distances = []\n",
    "    dict_relevant = evaluate_main_query(main_query, corpus, voc, ii, idfs)\n",
    "    \n",
    "    # for each synopsis that matches the full main query (a condition guaranteed by the \n",
    "    # \"if len(vector) == len(query_main_vector)\" check), we compute the cosine distance between the tfidfs\n",
    "    # and the tanimoto distance between the binary values for the parameters. Then we average these\n",
    "    # two values out and obtain a single score that we will put, together with the anime_num, in a tuple.\n",
    "    # It's important that the first value is the distance and not the anime_num because this makes heapify\n",
    "    # order the tuples correctly.\n",
    "    # The strategy of taking the negative of the final score seems to be the standard approach for obtaining\n",
    "    # a max heap from heapq.\n",
    "    for key in dict_relevant.keys():\n",
    "        vector = dict_relevant[key]\n",
    "        if len(vector) == len(query_main_vector): # this assures the conjuctive (and) property of the search engine\n",
    "            cosine = cosine_similarity(query_main_vector, vector)\n",
    "            vector_parameters = evaluate_parameters(query_dict, df, key)\n",
    "            tanimoto = tanimoto_distance(query_parameters_vector, vector_parameters)\n",
    "            distances.append((-np.mean([cosine, tanimoto]), key)) # here we negativize the mean to get a max heap \n",
    "    heapq.heapify(distances)\n",
    "    n_matches = len(distances)\n",
    "    final_results = []\n",
    "    for i in range(min(k, n_matches)):\n",
    "        el = heapq.heappop(distances)\n",
    "        final_results.append([el[1], -el[0]]) # make the cosine distance positive again for the output\n",
    "    \n",
    "    indices = [x[0] for x in final_results]\n",
    "    distances = [x[1] for x in final_results]\n",
    "    cols = [\"animeTitle\", \"animeDescription\"]\n",
    "    partial_df = df.iloc[indices][cols]\n",
    "    final_df = partial_df.assign(Url=[urls[i] for i in indices],\n",
    "                                 Similarity=distances)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***First Experiment***\n",
    "<br><br>\n",
    "So, for our first example we use the main query \"ranma urusei\" which should point more to 4286 (because that episode features both characters), and we provide the correct year and genre. Everything else is either wrong or missing. We end up with a similarity of approximately 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.798896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.555438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.798896  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.555438  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with query provided\n",
    "query = \"ranma urusei [year=2008, type=special, voices=(gianni), related=(gemelli del destino), characters=(ranma)]\"\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Second experiment***\n",
    "\n",
    "Now we keep the same main query but put the year for the other episode. This should raise the similarity for 1160 and lower the one for 4286. This is exactly what happens, but since the cosine similarity has more weight than the single coefficients for the tanimoto (so the parameters), the more similar is still the one that better matches the main query \"ranma urusei\", which I would argue is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.698896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.655438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.698896  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.655438  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ranma urusei [year=1995, type=special, voices=(gianni), related=(gemelli del destino), characters=(ranma)]\"\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Third experiment***\n",
    "\n",
    "Now we enter also the anyme type of the other episode. At this point, since we have two parameters pointing toward 1160, the order is inverted and we get 1160 as the first result. Again, this makes sense because at this point the query seems to match more 1160 than 4286."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.755438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.598896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "1160                                      Ranma ½ Super   \n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.755438  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.598896  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ranma urusei [year=1995, type=ova, voices=(gianni), related=(gemelli del destino), characters=(ranma)]\"\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Query retrieval via form***\n",
    "\n",
    "Here is an example of the query retrieval via form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: ranma urusei\n",
      "Year it was released: 2008\n",
      "Type of anime: special\n",
      "Voice actors: gianni\n",
      "Characters: sandro\n",
      "Related animes: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>It's a Rumic World: 50th Anniversary Weekly★Sh...</td>\n",
       "      <td>The characters from Ranma 1/2, Urusei Yatsura,...</td>\n",
       "      <td>https://myanimelist.net/anime/6566/Its_a_Rumic...</td>\n",
       "      <td>0.748896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ranma ½ Super</td>\n",
       "      <td>Super OVA 1: Based on a story from vol. 27 of ...</td>\n",
       "      <td>https://myanimelist.net/anime/1011/Ranma_½_Super</td>\n",
       "      <td>0.455438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             animeTitle  \\\n",
       "4286  It's a Rumic World: 50th Anniversary Weekly★Sh...   \n",
       "1160                                      Ranma ½ Super   \n",
       "\n",
       "                                       animeDescription  \\\n",
       "4286  The characters from Ranma 1/2, Urusei Yatsura,...   \n",
       "1160  Super OVA 1: Based on a story from vol. 27 of ...   \n",
       "\n",
       "                                                    Url  Similarity  \n",
       "4286  https://myanimelist.net/anime/6566/Its_a_Rumic...    0.748896  \n",
       "1160   https://myanimelist.net/anime/1011/Ranma_½_Super    0.455438  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with query via form\n",
    "search_k_matches_2(corpus, voc, ii_2, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                       1160\n",
       "animeTitle                                              Ranma ½ Super\n",
       "animeType                                                         OVA\n",
       "animeNumEpisode                                                   3.0\n",
       "releaseDate                                       1995-09-21 00:00:00\n",
       "endDate                                           1996-01-19 00:00:00\n",
       "animeNumMembers                                                 16985\n",
       "animeScore                                                       7.66\n",
       "animeUsers                                                     7855.0\n",
       "animeRank                                                      1162.0\n",
       "animePopularity                                                  4304\n",
       "animeDescription    Super OVA 1: Based on a story from vol. 27 of ...\n",
       "animeRelated                             ['Ranma ½: Yomigaeru Kioku']\n",
       "animeCharacters     ['Saotome, Ranma', 'Tendou, Akane', 'Hibiki, R...\n",
       "animeVoices         ['Hayashibara, Megumi', 'Hidaka, Noriko', 'Yam...\n",
       "animeStaff          [['Yamamoto, Harukichi', 'Theme Song Compositi...\n",
       "Name: 1160, dtype: object"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a *back-to-back sequence* of requests for appointments. A sequence of requests is of the form\n",
    "    > 30, 40, 25, 50, 30, 20\n",
    "where each number is the time that the person who makes the appointment wants to spend.\n",
    "You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, `[30, 50, 20]` is an acceptable solution (of duration *100*), but `[30, 40, 50, 20]` is not, because *30* and *40* are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration *110*.\n",
    "1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "\n",
    "##### Here we consider that all values in the instance are unique and len(instance) = n\n",
    "\n",
    "To compute the acceptable solution with the longest possible duration, we have to follow several steps :\n",
    "1. Compute every possible solution : So for that, list all sublists which represent each possible list of appointments.\n",
    "2. For each sublist, tell if it is acceptable or not, so if there are two consecutive appointments or not.\n",
    "3. Compute the total duration of each acceptable solution.\n",
    "4. Finally, return the solution which correspond to the maximum duration.\n",
    "\n",
    "```\n",
    "Input: \n",
    "    instance: list of length n\n",
    "\n",
    "function optimal_solution(instance):\n",
    "    n=len(instance)\n",
    "    for i=0 to n: \n",
    "        sublists = sublists + [all sublists with i elements]\n",
    "    \n",
    "    acceptable_solutions=[all element of sublists which are acceptable]\n",
    "    \n",
    "    durations = [duration of each element of acceptable_solutions]\n",
    "    max_duration = max(durations)\n",
    "    optimal_solutions = [sublists of instance with total duration == max_durations]\n",
    "    \n",
    "    return optimal_solutions, max_duration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement a program that given in input an instance in the form given above, gives the optimal solution.\n",
    "\n",
    "First of all, we create a function ```is_acceptable(solution, instance)``` that says if a solution is acceptable or not, i.e. there is not two consecutive requests of the instance in the solution.\n",
    "\n",
    "Then, the ```longest_acceptable_duration(instance)``` compute all possible sublists of the instance, test it with the function ```is_acceptable(solution, instance)```, sum every acceptable list (to compute the duration of each acceptable solution) and return the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(solution, instance):\n",
    "    res=True \n",
    "    for x,y in zip(solution[:-1],solution[1:]):\n",
    "        i= instance.index(x)\n",
    "        #index1 = np.where(np.array(instance) == x)\n",
    "        #for i in index[0]:\n",
    "        #if the next element is y \n",
    "        if instance[i+1] == y:\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "def optimal_solution(instance):\n",
    "    sublists=[]\n",
    "    \n",
    "    for i in range(1, len(instance)+1):\n",
    "        sublists+=[list(x) for x in combinations(instance, i)]\n",
    "        \n",
    "    mask = [is_acceptable(solution, instance) for solution in sublists]\n",
    "    acceptable_sol = np.array(sublists)[mask]\n",
    "    \n",
    "    durations = [sum(L) for L in acceptable_sol]\n",
    "    max_duration = max(durations)\n",
    "    \n",
    "    index_optimal_solutions = np.where(np.array(durations)==max_duration)\n",
    "    \n",
    "    return list(np.array(acceptable_sol)[index_optimal_solutions]), max_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40, 50, 20]] 110\n"
     ]
    }
   ],
   "source": [
    "instance = [30, 40, 25, 50, 30, 20]\n",
    "optimal_solutions, max_duration = optimal_solution(instance)\n",
    "print(optimal_solutions, max_duration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adm_hm3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
